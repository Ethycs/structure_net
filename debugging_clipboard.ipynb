{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517f1511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[32m⠁\u001b[0m activating environment                                                                 \u001b[32m⠁\u001b[0m                                                                               Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "🔄 Neuron sorting: Enabled (every 2 steps)\n",
      "🔬 Loading pretrained model from: ./data/promising_models/20250707_020152/model_cifar10_3layers_seed9_acc0.47_patch0.345_sparse0.050_BEST_ACCURACY_GLOBAL.pt\n",
      "/home/rabbit/structure_net/./experiments/hybrid_growth_experiment.py:578: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "   🎯 Using sparsity from checkpoint: 0.05\n",
      "   🏗️  Architecture: [3072, 2048, 10]\n",
      "   📊 Sparsity: 0.05\n",
      "   🔍 Available state dict keys:\n",
      "      0.linear.bias: torch.Size([2048])\n",
      "      0.linear.weight: torch.Size([2048, 3072])\n",
      "      0.mask: torch.Size([2048, 3072])\n",
      "      2.linear.bias: torch.Size([10])\n",
      "      2.linear.weight: torch.Size([10, 2048])\n",
      "      2.mask: torch.Size([10, 2048])\n",
      "   ✅ Loading pretrained weights for layer 0 (keys '0.linear.weight', '0.linear.bias')\n",
      "      ✅ Loaded original mask: 315349.0/6291456 connections\n",
      "   ✅ Loading pretrained weights for layer 1 (keys '2.linear.weight', '2.linear.bias')\n",
      "      ✅ Loaded original mask: 1082.0/20480 connections\n",
      "🧪 Testing initial accuracy...\n",
      "   🔍 Debug: Input shape: torch.Size([64, 3072])\n",
      "   🔍 Debug: Expected input dim: 3072\n",
      "   🔍 Debug: Target shape: torch.Size([64])\n",
      "   🔍 Debug: Layer 0 input shape: torch.Size([64, 3072])\n",
      "   🔍 Debug: Layer 0 weight shape: torch.Size([2048, 3072])\n",
      "   🔍 Debug: Layer 0 mask sum: 315349.0\n",
      "   🔍 Debug: Layer 0 output shape: torch.Size([64, 2048])\n",
      "   🔍 Debug: Layer 0 output stats: mean=0.0006, std=0.0660\n",
      "   🔍 Debug: Layer 1 input shape: torch.Size([64, 2048])\n",
      "   🔍 Debug: Layer 1 weight shape: torch.Size([10, 2048])\n",
      "   🔍 Debug: Layer 1 mask sum: 1082.0\n",
      "   🔍 Debug: Layer 1 output shape: torch.Size([64, 10])\n",
      "   🔍 Debug: Layer 1 output stats: mean=-0.0012, std=0.0137\n",
      "   🔍 Debug: Final output shape: torch.Size([64, 10])\n",
      "   🔍 Debug: Final output sample: tensor([ 0.0077, -0.0140,  0.0198,  0.0127,  0.0091], device='cuda:0')\n",
      "   📊 Initial accuracy: 1.56%\n",
      "🚀 Initialized OptimalGrowthEvolver (V2.0) with seed: [3072, 2048, 10], sparsity: 0.05\n",
      "   🔄 Neuron sorting: Enabled\n",
      "   📊 Sort frequency: Every 2 evolution steps\n",
      "\n",
      "🚀 Starting Delta-Guided Evolution for 3 steps...\n",
      "\n",
      "🧬 Evolution Step 1/3\n",
      "\n",
      "--- Analyzing Information Flow (MI) ---\n",
      "   MI Flow Detected: ['0.00', '0.00']\n",
      "No significant bottlenecks found. Evolution paused.\n",
      "   🔍 Debug: Input shape: torch.Size([64, 3072])\n",
      "   🔍 Debug: Expected input dim: 3072\n",
      "   🔍 Debug: Target shape: torch.Size([64])\n",
      "   🔍 Debug: Layer 0 input shape: torch.Size([64, 3072])\n",
      "   🔍 Debug: Layer 0 weight shape: torch.Size([2048, 3072])\n",
      "   🔍 Debug: Layer 0 mask sum: 315349.0\n",
      "   🔍 Debug: Layer 0 output shape: torch.Size([64, 2048])\n",
      "   🔍 Debug: Layer 0 output stats: mean=0.0006, std=0.0660\n",
      "   🔍 Debug: Layer 1 input shape: torch.Size([64, 2048])\n",
      "   🔍 Debug: Layer 1 weight shape: torch.Size([10, 2048])\n",
      "   🔍 Debug: Layer 1 mask sum: 1082.0\n",
      "   🔍 Debug: Layer 1 output shape: torch.Size([64, 10])\n",
      "   🔍 Debug: Layer 1 output stats: mean=-0.0012, std=0.0137\n",
      "   🔍 Debug: Final output shape: torch.Size([64, 10])\n",
      "   🔍 Debug: Final output sample: tensor([ 0.0077, -0.0140,  0.0198,  0.0127,  0.0091], device='cuda:0')\n",
      "📊 Accuracy after step 1: 1.56%\n",
      "\n",
      "🧬 Evolution Step 2/3\n",
      "   🔄 Performing neuron sorting maintenance...\n",
      "\n",
      "--- Analyzing Information Flow (MI) ---\n",
      "   MI Flow Detected: ['0.00', '0.00']\n",
      "No significant bottlenecks found. Evolution paused.\n",
      "   🔍 Debug: Input shape: torch.Size([64, 3072])\n",
      "   🔍 Debug: Expected input dim: 3072\n",
      "   🔍 Debug: Target shape: torch.Size([64])\n",
      "   🔍 Debug: Layer 0 input shape: torch.Size([64, 3072])\n",
      "   🔍 Debug: Layer 0 weight shape: torch.Size([2048, 3072])\n",
      "   🔍 Debug: Layer 0 mask sum: 315349.0\n",
      "   🔍 Debug: Layer 0 output shape: torch.Size([64, 2048])\n",
      "   🔍 Debug: Layer 0 output stats: mean=0.0006, std=0.0660\n",
      "   🔍 Debug: Layer 1 input shape: torch.Size([64, 2048])\n",
      "   🔍 Debug: Layer 1 weight shape: torch.Size([10, 2048])\n",
      "   🔍 Debug: Layer 1 mask sum: 1082.0\n",
      "   🔍 Debug: Layer 1 output shape: torch.Size([64, 10])\n",
      "   🔍 Debug: Layer 1 output stats: mean=-0.0012, std=0.0137\n",
      "   🔍 Debug: Final output shape: torch.Size([64, 10])\n",
      "   🔍 Debug: Final output sample: tensor([ 0.0077, -0.0140,  0.0198,  0.0127,  0.0091], device='cuda:0')\n",
      "📊 Accuracy after step 2: 1.56%\n",
      "\n",
      "🧬 Evolution Step 3/3\n",
      "\n",
      "--- Analyzing Information Flow (MI) ---\n",
      "   MI Flow Detected: ['0.00', '0.00']\n",
      "No significant bottlenecks found. Evolution paused.\n",
      "   🔍 Debug: Input shape: torch.Size([64, 3072])\n",
      "   🔍 Debug: Expected input dim: 3072\n",
      "   🔍 Debug: Target shape: torch.Size([64])\n",
      "   🔍 Debug: Layer 0 input shape: torch.Size([64, 3072])\n",
      "   🔍 Debug: Layer 0 weight shape: torch.Size([2048, 3072])\n",
      "   🔍 Debug: Layer 0 mask sum: 315349.0\n",
      "   🔍 Debug: Layer 0 output shape: torch.Size([64, 2048])\n",
      "   🔍 Debug: Layer 0 output stats: mean=0.0006, std=0.0660\n",
      "   🔍 Debug: Layer 1 input shape: torch.Size([64, 2048])\n",
      "   🔍 Debug: Layer 1 weight shape: torch.Size([10, 2048])\n",
      "   🔍 Debug: Layer 1 mask sum: 1082.0\n",
      "   🔍 Debug: Layer 1 output shape: torch.Size([64, 10])\n",
      "   🔍 Debug: Layer 1 output stats: mean=-0.0012, std=0.0137\n",
      "   🔍 Debug: Final output shape: torch.Size([64, 10])\n",
      "   🔍 Debug: Final output sample: tensor([ 0.0077, -0.0140,  0.0198,  0.0127,  0.0091], device='cuda:0')\n",
      "📊 Accuracy after step 3: 1.56%\n",
      "\n",
      "✅ Evolution complete!\n",
      "📈 Evolution history:\n",
      "   1. Performed neuron sorting at evolution step 2\n"
     ]
    }
   ],
   "source": [
    "!pixi run CUDA_VISIBLE_DEVICES=1 pixi run python ./experiments/hybrid_growth_experiment.py --load-model ./data/promising_models/20250707_020152/model_cifar10_3layers_seed9_acc0.47_patch0.345_sparse0.050_BEST_ACCURACY_GLOBAL.pt --evolution-steps 3 --sort-frequency 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa4d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Physical GPU 1 ('NVIDIA GeForce RTX 2060 SUPER') is now active as PyTorch device 'cuda:0'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "def setup_correct_device(physical_gpu_id):\n",
    "    \"\"\"\n",
    "    Correctly selects a physical GPU and makes it the ONLY one visible to PyTorch.\n",
    "    This function preserves the mapping you expect.\n",
    "    \"\"\"\n",
    "    # 1. Set the environment variable using the physical ID.\n",
    "    #    This MUST be done before any torch.cuda calls.\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(physical_gpu_id)\n",
    "\n",
    "    # 2. Now that PyTorch only sees one GPU, it will always be 'cuda:0'.\n",
    "    if torch.cuda.is_available():\n",
    "        # Get the name to confirm we got the right one.\n",
    "        gpu_name = torch.cuda.get_device_name(0) \n",
    "        print(f\"✅ Success! Physical GPU {physical_gpu_id} ('{gpu_name}') is now active as PyTorch device 'cuda:0'.\")\n",
    "        return torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        print(\"⚠️ CUDA not available. Falling back to CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = setup_correct_device(1)\n",
    "\n",
    "def inspect_checkpoint(checkpoint_path):\n",
    "    \"\"\"Comprehensive checkpoint inspector\"\"\"\n",
    "    print(f\"\\n🔍 Inspecting: {checkpoint_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # 1. Basic info\n",
    "    print(\"\\n📋 Checkpoint Keys:\")\n",
    "    for key in checkpoint.keys():\n",
    "        if isinstance(checkpoint[key], torch.Tensor):\n",
    "            print(f\"  {key}: {checkpoint[key].shape}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {checkpoint[key]}\")\n",
    "    \n",
    "    # 2. Model state dict analysis\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        print(\"\\n🏗️  Model Architecture:\")\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        # Analyze layers\n",
    "        layers = {}\n",
    "        for key, tensor in state_dict.items():\n",
    "            layer_name = key.split('.')[0]\n",
    "            if layer_name not in layers:\n",
    "                layers[layer_name] = {}\n",
    "            layers[layer_name][key] = tensor\n",
    "        \n",
    "        for layer_name in sorted(layers.keys()):\n",
    "            print(f\"\\n  Layer {layer_name}:\")\n",
    "            for key, tensor in layers[layer_name].items():\n",
    "                print(f\"    {key}: {tensor.shape}\")\n",
    "                \n",
    "                # Statistics for weights\n",
    "                if 'weight' in key:\n",
    "                    non_zero = (tensor != 0).sum().item()\n",
    "                    total = tensor.numel()\n",
    "                    sparsity = 1 - (non_zero / total)\n",
    "                    print(f\"      Non-zero: {non_zero}/{total} (sparsity: {sparsity:.2%})\")\n",
    "                    print(f\"      Stats: mean={tensor.mean():.4f}, std={tensor.std():.4f}\")\n",
    "                    print(f\"      Range: [{tensor.min():.4f}, {tensor.max():.4f}]\")\n",
    "    \n",
    "    # 3. Training info\n",
    "    print(\"\\n📊 Training Info:\")\n",
    "    for key in ['accuracy', 'epoch', 'loss', 'sparsity']:\n",
    "        if key in checkpoint:\n",
    "            print(f\"  {key}: {checkpoint[key]}\")\n",
    "    \n",
    "    # 4. Architecture if available\n",
    "    if 'architecture' in checkpoint:\n",
    "        print(f\"\\n🏛️  Architecture: {checkpoint['architecture']}\")\n",
    "    \n",
    "    # 5. Any optimizer state\n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        print(\"\\n📈 Optimizer state: Present\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Quick usage function\n",
    "def quick_inspect(path):\n",
    "    \"\"\"Just the essentials\"\"\"\n",
    "    ckpt = torch.load(path, map_location='cpu')\n",
    "    print(f\"\\n{path.split('/')[-1]}:\")\n",
    "    print(f\"  Keys: {list(ckpt.keys())}\")\n",
    "    if 'accuracy' in ckpt:\n",
    "        print(f\"  Accuracy: {ckpt['accuracy']}\")\n",
    "    if 'architecture' in ckpt:\n",
    "        print(f\"  Architecture: {ckpt['architecture']}\")\n",
    "    if 'model_state_dict' in ckpt:\n",
    "        total_params = sum(p.numel() for p in ckpt['model_state_dict'].values() if 'weight' in p)\n",
    "        print(f\"  Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d87e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Inspecting: data/promising_models/20250707_020152/model_cifar10_3layers_seed9_acc0.47_patch0.345_sparse0.050_BEST_ACCURACY_GLOBAL.pt\n",
      "============================================================\n",
      "\n",
      "📋 Checkpoint Keys:\n",
      "  model_state_dict: OrderedDict([('0.mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('0.linear.weight', tensor([[0., -0., -0.,  ..., 0., -0., 0.],\n",
      "        [0., -0., -0.,  ..., 0., 0., -0.],\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        ...,\n",
      "        [-0., -0., 0.,  ..., -0., -0., -0.],\n",
      "        [-0., -0., 0.,  ..., -0., -0., 0.],\n",
      "        [0., 0., 0.,  ..., -0., -0., -0.]])), ('0.linear.bias', tensor([0.0137, 0.0165, 0.0170,  ..., 0.0012, 0.0162, 0.0020])), ('2.mask', tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('2.linear.weight', tensor([[-0.0111,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0058, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        ...,\n",
      "        [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "        [-0.0008, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]])), ('2.linear.bias', tensor([ 0.0069, -0.0100,  0.0202,  0.0095,  0.0084,  0.0128, -0.0217, -0.0073,\n",
      "        -0.0153, -0.0104]))])\n",
      "  architecture: [3072, 2048, 10]\n",
      "  seed: 9\n",
      "  sparsity: 0.05\n",
      "  epoch: 5\n",
      "  optimizer_state_dict: None\n",
      "  accuracy: 0.4735\n",
      "  patchability_score: 0.34508715820312497\n",
      "  extrema_counts: 0.6554361979166666\n",
      "  efficiency: 7.49921523523779e-08\n",
      "  neuron_sorting_enabled: True\n",
      "  sort_frequency: 5\n",
      "  training_epochs: 5\n",
      "  dead_neurons: 0\n",
      "  saturated_neurons: 0\n",
      "  activation_patterns: None\n",
      "  torch_version: 2.5.1+cu121\n",
      "  random_state: torch.Size([5056])\n",
      "  cuda_random_state: torch.Size([16])\n",
      "\n",
      "🏗️  Model Architecture:\n",
      "\n",
      "  Layer 0:\n",
      "    0.mask: torch.Size([2048, 3072])\n",
      "    0.linear.weight: torch.Size([2048, 3072])\n",
      "      Non-zero: 315349/6291456 (sparsity: 94.99%)\n",
      "      Stats: mean=-0.0000, std=0.0023\n",
      "      Range: [-0.0180, 0.0180]\n",
      "    0.linear.bias: torch.Size([2048])\n",
      "\n",
      "  Layer 2:\n",
      "    2.mask: torch.Size([10, 2048])\n",
      "    2.linear.weight: torch.Size([10, 2048])\n",
      "      Non-zero: 1082/20480 (sparsity: 94.72%)\n",
      "      Stats: mean=-0.0000, std=0.0029\n",
      "      Range: [-0.0221, 0.0220]\n",
      "    2.linear.bias: torch.Size([10])\n",
      "\n",
      "📊 Training Info:\n",
      "  accuracy: 0.4735\n",
      "  epoch: 5\n",
      "  sparsity: 0.05\n",
      "\n",
      "🏛️  Architecture: [3072, 2048, 10]\n",
      "\n",
      "📈 Optimizer state: Present\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_466749/2165641061.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"data/promising_models/20250707_020152/model_cifar10_3layers_seed9_acc0.47_patch0.345_sparse0.050_BEST_ACCURACY_GLOBAL.pt\"\n",
    "inspect_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7b1fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Physical GPU 1 ('NVIDIA GeForce RTX 2060 SUPER') is now active as PyTorch device 'cuda:0'.\n",
      "Checkpoint claims accuracy: 0.4735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_466749/1060586722.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"0.weight\", \"0.bias\", \"2.weight\", \"2.bias\". \n\tUnexpected key(s) in state_dict: \"0.mask\", \"0.linear.weight\", \"0.linear.bias\", \"2.mask\", \"2.linear.weight\", \"2.linear.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m         model.add_module(\u001b[38;5;28mstr\u001b[39m(i*\u001b[32m2\u001b[39m+\u001b[32m1\u001b[39m), nn.ReLU())\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Load weights EXACTLY as saved\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_state_dict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Test it\u001b[39;00m\n\u001b[32m     21\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/.pixi/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2576\u001b[39m         error_msgs.insert(\n\u001b[32m   2577\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2578\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2579\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2580\u001b[39m             ),\n\u001b[32m   2581\u001b[39m         )\n\u001b[32m   2583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2586\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2587\u001b[39m         )\n\u001b[32m   2588\u001b[39m     )\n\u001b[32m   2589\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"0.weight\", \"0.bias\", \"2.weight\", \"2.bias\". \n\tUnexpected key(s) in state_dict: \"0.mask\", \"0.linear.weight\", \"0.linear.bias\", \"2.mask\", \"2.linear.weight\", \"2.linear.bias\". "
     ]
    }
   ],
   "source": [
    "# Test EXACTLY as the hunter would\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = setup_correct_device(1)\n",
    "# Load checkpoint\n",
    "ckpt = torch.load(checkpoint_path)\n",
    "print(f\"Checkpoint claims accuracy: {ckpt.get('accuracy', 'NOT FOUND')}\")\n",
    "\n",
    "# Recreate EXACT model from hunter\n",
    "model = nn.Sequential()\n",
    "arch = ckpt['architecture']\n",
    "for i in range(len(arch)-1):\n",
    "    model.add_module(str(i*2), nn.Linear(arch[i], arch[i+1]))\n",
    "    if i < len(arch)-2:\n",
    "        model.add_module(str(i*2+1), nn.ReLU())\n",
    "\n",
    "# Load weights EXACTLY as saved\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "# Test it\n",
    "model.eval()\n",
    "# ... test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d2bed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Physical GPU 1 ('NVIDIA GeForce RTX 2060 SUPER') is now active as PyTorch device 'cuda:0'.\n",
      "Using device: cuda:0\n",
      "Files already downloaded and verified\n",
      "Testing model on CIFAR-10...\n",
      "\n",
      "✅ Model loaded! Architecture: [3072, 2048, 10]\n",
      "📊 Claimed accuracy: 47.35%\n",
      "🔍 Actual tested accuracy: 9.76%\n",
      "❓ Match? False\n",
      "\n",
      "🚨 Accuracy suspiciously low! Debugging...\n",
      "0.linear.weight device: cuda:0\n",
      "0.linear.bias device: cuda:0\n",
      "2.linear.weight device: cuda:0\n",
      "2.linear.bias device: cuda:0\n",
      "0.mask device: cuda:0\n",
      "2.mask device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "device = setup_correct_device(1)\n",
    "# Set CUDA devices BEFORE importing torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2\" # This is fine, but for simplicity let's let PyTorch manage it\n",
    "\n",
    "class SparseLayer(nn.Module):\n",
    "    \"\"\"Matches what the hunter used\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        # Initialize mask properly\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # The mask will be on the same device as the weight because both are part of the model\n",
    "        return F.linear(x, self.linear.weight * self.mask, self.linear.bias)\n",
    "\n",
    "# CIFAR-10 test loader\n",
    "def get_cifar10_test_loader(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=False,\n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = \"./data/promising_models/20250707_020152/model_cifar10_3layers_seed9_acc0.47_patch0.345_sparse0.050_BEST_ACCURACY_GLOBAL.pt\"\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load to CPU first to prevent device mismatches during loading\n",
    "# Using weights_only=True is safer for untrusted files\n",
    "ckpt = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "# Build model\n",
    "arch = ckpt['architecture']\n",
    "model = nn.Sequential()\n",
    "\n",
    "for i in range(len(arch)-1):\n",
    "    layer = SparseLayer(arch[i], arch[i+1])\n",
    "    \n",
    "    # Get keys for the state dict\n",
    "    weight_key = f'{i*2}.linear.weight'\n",
    "    bias_key = f'{i*2}.linear.bias'\n",
    "    mask_key = f'{i*2}.mask'\n",
    "    \n",
    "    # Load the weights and mask data\n",
    "    if weight_key in ckpt['model_state_dict']:\n",
    "        layer.linear.weight.data = ckpt['model_state_dict'][weight_key]\n",
    "        layer.linear.bias.data = ckpt['model_state_dict'][bias_key]\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        # Load data INTO the buffer, don't replace the buffer object itself.\n",
    "        layer.mask.data = ckpt['model_state_dict'][mask_key]\n",
    "    \n",
    "    model.add_module(str(i*2), layer)\n",
    "    if i < len(arch)-2:\n",
    "        model.add_module(str(i*2+1), nn.ReLU())\n",
    "\n",
    "# NOW move the entire model (including registered buffers) to the correct device\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Get test loader\n",
    "test_loader = get_cifar10_test_loader()\n",
    "\n",
    "# Test accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(\"Testing model on CIFAR-10...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        # Move data for the current batch to the device\n",
    "        data = data.view(data.size(0), -1).to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += len(target)\n",
    "        \n",
    "        # Optional: Break early for a quick check\n",
    "        # if total >= 1000:\n",
    "        #     break\n",
    "\n",
    "actual_accuracy = correct / total\n",
    "print(f\"\\n✅ Model loaded! Architecture: {ckpt['architecture']}\")\n",
    "print(f\"📊 Claimed accuracy: {ckpt['accuracy']:.2%}\")\n",
    "print(f\"🔍 Actual tested accuracy: {actual_accuracy:.2%}\")\n",
    "print(f\"❓ Match? {abs(actual_accuracy - ckpt['accuracy']) < 0.05}\")\n",
    "\n",
    "if actual_accuracy < 0.1:\n",
    "    print(\"\\n🚨 Accuracy suspiciously low! Debugging...\")\n",
    "    # Add device check to confirm the fix\n",
    "    for name, p in model.named_parameters():\n",
    "        print(f\"{name} device: {p.device}\")\n",
    "    for name, b in model.named_buffers():\n",
    "        print(f\"{name} device: {b.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0d4d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Debugging weight statistics:\n",
      "0.linear.weight:\n",
      "  Shape: torch.Size([2048, 3072])\n",
      "  Non-zero: 315349/6291456 (5.01%)\n",
      "  Mean: -0.000002, Std: 0.002333\n",
      "  Max: 0.018042, Min: -0.018042\n",
      "2.linear.weight:\n",
      "  Shape: torch.Size([10, 2048])\n",
      "  Non-zero: 1082/20480 (5.28%)\n",
      "  Mean: -0.000019, Std: 0.002931\n",
      "  Max: 0.021964, Min: -0.022068\n",
      "\n",
      "🔍 Debugging mask statistics:\n",
      "0.mask: 315349.0/6291456 active (5.01%)\n",
      "2.mask: 1082.0/20480 active (5.28%)\n",
      "\n",
      "🔍 Testing raw output:\n",
      "After layer 0: mean=0.001343, std=0.129675\n",
      "After ReLU: mean=0.052195, std=0.075405\n",
      "Final output: tensor([[ 0.0064, -0.0011,  0.0100,  0.0042, -0.0057, -0.0154, -0.0239, -0.0168,\n",
      "         -0.0139, -0.0348]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Debug the loaded weights\n",
    "print(\"\\n🔍 Debugging weight statistics:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        non_zero = (param != 0).sum().item()\n",
    "        total = param.numel()\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Shape: {param.shape}\")\n",
    "        print(f\"  Non-zero: {non_zero}/{total} ({non_zero/total*100:.2f}%)\")\n",
    "        print(f\"  Mean: {param.mean():.6f}, Std: {param.std():.6f}\")\n",
    "        print(f\"  Max: {param.max():.6f}, Min: {param.min():.6f}\")\n",
    "\n",
    "# Also check the masks\n",
    "print(\"\\n🔍 Debugging mask statistics:\")\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'mask'):\n",
    "        mask = module.mask\n",
    "        active = mask.sum().item()\n",
    "        total = mask.numel()\n",
    "        print(f\"{name}.mask: {active}/{total} active ({active/total*100:.2f}%)\")\n",
    "\n",
    "# Test with the actual weights as they are\n",
    "print(\"\\n🔍 Testing raw output:\")\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(1, 3072).to(device)\n",
    "    layer0_out = model[0](test_input)\n",
    "    print(f\"After layer 0: mean={layer0_out.mean():.6f}, std={layer0_out.std():.6f}\")\n",
    "    layer1_out = F.relu(layer0_out)\n",
    "    print(f\"After ReLU: mean={layer1_out.mean():.6f}, std={layer1_out.std():.6f}\")\n",
    "    final_out = model[2](layer1_out)\n",
    "    print(f\"Final output: {final_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f47c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checkpoint architecture analysis:\n",
      "Architecture from metadata: [3072, 2048, 10]\n",
      "\n",
      "Actual layers in state dict:\n",
      "  Layer 0: torch.Size([2048, 3072])\n",
      "  Layer 2: torch.Size([10, 2048])\n",
      "\n",
      "⚠️ Architecture has 3 values = 2 layers, but filename says '3layers'\n",
      "This might be a naming inconsistency.\n"
     ]
    }
   ],
   "source": [
    "# Print what's actually in the checkpoint\n",
    "print(\"🔍 Checkpoint architecture analysis:\")\n",
    "print(f\"Architecture from metadata: {ckpt['architecture']}\")\n",
    "print(\"\\nActual layers in state dict:\")\n",
    "for key in sorted(ckpt['model_state_dict'].keys()):\n",
    "    if 'weight' in key:\n",
    "        layer_idx = key.split('.')[0]\n",
    "        shape = ckpt['model_state_dict'][key].shape\n",
    "        print(f\"  Layer {layer_idx}: {shape}\")\n",
    "\n",
    "# The architecture [3072, 2048, 10] only has 2 layers!\n",
    "# But the filename says 3 layers...\n",
    "\n",
    "# Let's try assuming there's a missing middle layer\n",
    "# Maybe the architecture should be [3072, ?, ?, 10]?\n",
    "\n",
    "# Check if this is actually a 2-layer network mislabeled as 3\n",
    "if len(ckpt['architecture']) == 3:\n",
    "    print(\"\\n⚠️ Architecture has 3 values = 2 layers, but filename says '3layers'\")\n",
    "    print(\"This might be a naming inconsistency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3a1b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Plain 2-layer model accuracy: 8.12%\n"
     ]
    }
   ],
   "source": [
    "# Just load it as a 2-layer network and test\n",
    "model_2layer = nn.Sequential(\n",
    "    nn.Linear(3072, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2048, 10)\n",
    ")\n",
    "\n",
    "# Load the weights (without masks for now)\n",
    "model_2layer[0].weight.data = ckpt['model_state_dict']['0.linear.weight']\n",
    "model_2layer[0].bias.data = ckpt['model_state_dict']['0.linear.bias']\n",
    "model_2layer[2].weight.data = ckpt['model_state_dict']['2.linear.weight']\n",
    "model_2layer[2].bias.data = ckpt['model_state_dict']['2.linear.bias']\n",
    "\n",
    "model_2layer = model_2layer.to(device)\n",
    "model_2layer.eval()\n",
    "\n",
    "# Quick test\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        if i >= 10:  # Just 10 batches\n",
    "            break\n",
    "        data = data.view(data.size(0), -1).to(device)\n",
    "        output = model_2layer(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target.to(device)).sum().item()\n",
    "\n",
    "print(f\"\\n🎯 Plain 2-layer model accuracy: {correct/640:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ae55a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing modular structure_net...\n",
      "✅ Network created: 3 layers\n",
      "✅ Network stats: [784, 128, 10]\n",
      "✅ LSUV initialization successful\n",
      "🎯 All modular components working!\n"
     ]
    }
   ],
   "source": [
    "from src.structure_net import create_standard_network, get_network_stats, lsuv_init_network\n",
    "import torch\n",
    "\n",
    "print('🧪 Testing modular structure_net...')\n",
    "\n",
    "# Test network creation\n",
    "network = create_standard_network([784, 128, 10], 0.02, device='cpu')\n",
    "print(f'✅ Network created: {len(network)} layers')\n",
    "\n",
    "# Test network stats\n",
    "stats = get_network_stats(network)\n",
    "print(f'✅ Network stats: {stats[\"architecture\"]}')\n",
    "\n",
    "# Test LSUV\n",
    "sample_batch = torch.randn(32, 784)\n",
    "lsuv_init_network(network, sample_batch, verbose=False)\n",
    "print('✅ LSUV initialization successful')\n",
    "\n",
    "print('🎯 All modular components working!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2083c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.structure_net.evolution.integrated_growth_system:🔍 Analyzing network bottlenecks with exact MI...\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:Computing EXACT MI: torch.Size([48, 64]) → torch.Size([48, 64]) active neurons\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:Computing EXACT MI: torch.Size([48, 32]) → torch.Size([48, 18]) active neurons\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing integrated growth system...\n",
      "✅ Network created: 5 layers\n",
      "🔍 Testing network analysis...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'math'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m config.activation_threshold = \u001b[32m0.01\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m🔍 Testing network analysis...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m improved_network = \u001b[43manalyze_and_grow_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_growth_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_analysis_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m✅ Analysis complete! Network type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(improved_network)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m🎯 Integrated growth system working!\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:638\u001b[39m, in \u001b[36manalyze_and_grow_network\u001b[39m\u001b[34m(network, train_loader, val_loader, threshold_config, max_growth_actions, num_analysis_batches)\u001b[39m\n\u001b[32m    635\u001b[39m growth_system = StructureNetGrowthSystem(network, threshold_config)\n\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# Analyze network\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m analysis_results = \u001b[43mgrowth_system\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_network_bottlenecks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_analysis_batches\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[38;5;66;03m# Apply improvements\u001b[39;00m\n\u001b[32m    643\u001b[39m improved_network = growth_system.apply_growth_recommendations(\n\u001b[32m    644\u001b[39m     analysis_results[\u001b[33m'\u001b[39m\u001b[33mrecommendations\u001b[39m\u001b[33m'\u001b[39m], max_growth_actions\n\u001b[32m    645\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:337\u001b[39m, in \u001b[36mStructureNetGrowthSystem.analyze_network_bottlenecks\u001b[39m\u001b[34m(self, data_loader, num_batches)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# Analyze each layer pair\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sparse_layers) - \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     layer_pair_analysis = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_analyze_layer_pair\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m     analysis_results[\u001b[33m'\u001b[39m\u001b[33mlayer_analyses\u001b[39m\u001b[33m'\u001b[39m][\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] = layer_pair_analysis\n\u001b[32m    342\u001b[39m     \u001b[38;5;66;03m# Identify bottlenecks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:415\u001b[39m, in \u001b[36mStructureNetGrowthSystem._analyze_layer_pair\u001b[39m\u001b[34m(self, layer_i, layer_j, data_loader, num_batches)\u001b[39m\n\u001b[32m    412\u001b[39m     acts_j = F.relu(acts_j)\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# Compute exact MI\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m mi_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexact_mi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_exact_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43macts_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macts_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# Compute dead neuron ratios\u001b[39;00m\n\u001b[32m    418\u001b[39m active_mask_i = (acts_i.abs() > \u001b[38;5;28mself\u001b[39m.threshold_config.activation_threshold).any(dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:108\u001b[39m, in \u001b[36mExactMutualInformation.compute_exact_mi\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    105\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mexact_discrete\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m n_active_X + n_active_Y <= \u001b[32m50\u001b[39m:\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# Use k-NN estimator (essentially exact for low dimensions)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_knn_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_active\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_active\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mknn_exact\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# Still manageable with advanced estimators\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:191\u001b[39m, in \u001b[36mExactMutualInformation._knn_mi\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    188\u001b[39m mi = np.mean(mi_values)\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Estimate entropies using k-NN\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m h_x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_knn_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m h_y = \u001b[38;5;28mself\u001b[39m._knn_entropy(Y_np)\n\u001b[32m    193\u001b[39m h_xy = \u001b[38;5;28mself\u001b[39m._knn_entropy(np.hstack([X_np, Y_np]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:220\u001b[39m, in \u001b[36mExactMutualInformation._knn_entropy\u001b[39m\u001b[34m(self, X, k)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# Kozachenko-Leonenko estimator\u001b[39;00m\n\u001b[32m    219\u001b[39m d = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m volume = (np.pi ** (d/\u001b[32m2\u001b[39m)) / np.exp(np.log(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmath\u001b[49m.gamma(d/\u001b[32m2\u001b[39m + \u001b[32m1\u001b[39m)))\n\u001b[32m    222\u001b[39m h = np.log(rho + \u001b[32m1e-10\u001b[39m).mean() * d + np.log(volume) + np.log(X.shape[\u001b[32m0\u001b[39m]) - np.log(k)\n\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# Convert to bits\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/.pixi/envs/default/lib/python3.11/site-packages/numpy/__init__.py:795\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchar\u001b[39;00m\n\u001b[32m    793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m char.chararray\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute 'math'"
     ]
    }
   ],
   "source": [
    "from src.structure_net import create_standard_network, analyze_and_grow_network, ThresholdConfig\n",
    "import torch\n",
    "\n",
    "print('🧪 Testing integrated growth system...')\n",
    "\n",
    "# Create a test network\n",
    "network = create_standard_network([784, 64, 32, 10], 0.02, device='cpu')\n",
    "print(f'✅ Network created: {len(network)} layers')\n",
    "\n",
    "# Create dummy data loader\n",
    "train_data = torch.randn(100, 784)\n",
    "train_labels = torch.randint(0, 10, (100,))\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "# Test the integrated growth system\n",
    "config = ThresholdConfig()\n",
    "config.activation_threshold = 0.01\n",
    "\n",
    "print('🔍 Testing network analysis...')\n",
    "improved_network = analyze_and_grow_network(\n",
    "    network, \n",
    "    train_loader, \n",
    "    threshold_config=config,\n",
    "    max_growth_actions=2,\n",
    "    num_analysis_batches=3\n",
    ")\n",
    "\n",
    "print(f'✅ Analysis complete! Network type: {type(improved_network)}')\n",
    "print('🎯 Integrated growth system working!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.structure_net.evolution.integrated_growth_system:🔍 Analyzing network bottlenecks with exact MI...\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:Computing EXACT MI: torch.Size([48, 64]) → torch.Size([48, 64]) active neurons\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:Computing EXACT MI: torch.Size([48, 32]) → torch.Size([48, 18]) active neurons\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing integrated growth system...\n",
      "✅ Network created: 5 layers\n",
      "🔍 Testing network analysis...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'math'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n",
      "\u001b[32m     18\u001b[39m config.activation_threshold = \u001b[32m0.01\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m🔍 Testing network analysis...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m improved_network = \u001b[43manalyze_and_grow_network\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_growth_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_analysis_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n",
      "\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m✅ Analysis complete! Network type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(improved_network)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m🎯 Integrated growth system working!\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:638\u001b[39m, in \u001b[36manalyze_and_grow_network\u001b[39m\u001b[34m(network, train_loader, val_loader, threshold_config, max_growth_actions, num_analysis_batches)\u001b[39m\n",
      "\u001b[32m    635\u001b[39m growth_system = StructureNetGrowthSystem(network, threshold_config)\n",
      "\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# Analyze network\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m analysis_results = \u001b[43mgrowth_system\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_network_bottlenecks\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_analysis_batches\u001b[49m\n",
      "\u001b[32m    640\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    642\u001b[39m \u001b[38;5;66;03m# Apply improvements\u001b[39;00m\n",
      "\u001b[32m    643\u001b[39m improved_network = growth_system.apply_growth_recommendations(\n",
      "\u001b[32m    644\u001b[39m     analysis_results[\u001b[33m'\u001b[39m\u001b[33mrecommendations\u001b[39m\u001b[33m'\u001b[39m], max_growth_actions\n",
      "\u001b[32m    645\u001b[39m )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:337\u001b[39m, in \u001b[36mStructureNetGrowthSystem.analyze_network_bottlenecks\u001b[39m\u001b[34m(self, data_loader, num_batches)\u001b[39m\n",
      "\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# Analyze each layer pair\u001b[39;00m\n",
      "\u001b[32m    336\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sparse_layers) - \u001b[32m1\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     layer_pair_analysis = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_analyze_layer_pair\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\n",
      "\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    340\u001b[39m     analysis_results[\u001b[33m'\u001b[39m\u001b[33mlayer_analyses\u001b[39m\u001b[33m'\u001b[39m][\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] = layer_pair_analysis\n",
      "\u001b[32m    342\u001b[39m     \u001b[38;5;66;03m# Identify bottlenecks\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:415\u001b[39m, in \u001b[36mStructureNetGrowthSystem._analyze_layer_pair\u001b[39m\u001b[34m(self, layer_i, layer_j, data_loader, num_batches)\u001b[39m\n",
      "\u001b[32m    412\u001b[39m     acts_j = F.relu(acts_j)\n",
      "\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# Compute exact MI\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m mi_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexact_mi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_exact_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43macts_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macts_j\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# Compute dead neuron ratios\u001b[39;00m\n",
      "\u001b[32m    418\u001b[39m active_mask_i = (acts_i.abs() > \u001b[38;5;28mself\u001b[39m.threshold_config.activation_threshold).any(dim=\u001b[32m0\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:108\u001b[39m, in \u001b[36mExactMutualInformation.compute_exact_mi\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n",
      "\u001b[32m    105\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mexact_discrete\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[32m    106\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m n_active_X + n_active_Y <= \u001b[32m50\u001b[39m:\n",
      "\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# Use k-NN estimator (essentially exact for low dimensions)\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_knn_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_active\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_active\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    109\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mknn_exact\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[32m    110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# Still manageable with advanced estimators\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:191\u001b[39m, in \u001b[36mExactMutualInformation._knn_mi\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n",
      "\u001b[32m    188\u001b[39m mi = np.mean(mi_values)\n",
      "\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Estimate entropies using k-NN\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m h_x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_knn_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_np\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    192\u001b[39m h_y = \u001b[38;5;28mself\u001b[39m._knn_entropy(Y_np)\n",
      "\u001b[32m    193\u001b[39m h_xy = \u001b[38;5;28mself\u001b[39m._knn_entropy(np.hstack([X_np, Y_np]))\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:220\u001b[39m, in \u001b[36mExactMutualInformation._knn_entropy\u001b[39m\u001b[34m(self, X, k)\u001b[39m\n",
      "\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# Kozachenko-Leonenko estimator\u001b[39;00m\n",
      "\u001b[32m    219\u001b[39m d = X.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m volume = (np.pi ** (d/\u001b[32m2\u001b[39m)) / np.exp(np.log(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmath\u001b[49m.gamma(d/\u001b[32m2\u001b[39m + \u001b[32m1\u001b[39m)))\n",
      "\u001b[32m    222\u001b[39m h = np.log(rho + \u001b[32m1e-10\u001b[39m).mean() * d + np.log(volume) + np.log(X.shape[\u001b[32m0\u001b[39m]) - np.log(k)\n",
      "\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# Convert to bits\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/.pixi/envs/default/lib/python3.11/site-packages/numpy/__init__.py:795\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n",
      "\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchar\u001b[39;00m\n",
      "\u001b[32m    793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m char.chararray\n",
      "\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute 'math'"
     ]
    }
   ],
   "source": [
    "from src.structure_net import create_standard_network, analyze_and_grow_network, ThresholdConfig\n",
    "import torch\n",
    "\n",
    "print('🧪 Testing integrated growth system...')\n",
    "\n",
    "# Create a test network\n",
    "network = create_standard_network([784, 64, 32, 10], 0.02, device='cpu')\n",
    "print(f'✅ Network created: {len(network)} layers')\n",
    "\n",
    "# Create dummy data loader\n",
    "train_data = torch.randn(100, 784)\n",
    "train_labels = torch.randint(0, 10, (100,))\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "# Test the integrated growth system\n",
    "config = ThresholdConfig()\n",
    "config.activation_threshold = 0.01\n",
    "\n",
    "print('🔍 Testing network analysis...')\n",
    "improved_network = analyze_and_grow_network(\n",
    "    network, \n",
    "    train_loader, \n",
    "    threshold_config=config,\n",
    "    max_growth_actions=2,\n",
    "    num_analysis_batches=3\n",
    ")\n",
    "\n",
    "print(f'✅ Analysis complete! Network type: {type(improved_network)}')\n",
    "print('🎯 Integrated growth system working!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec9c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.structure_net.evolution.integrated_growth_system:🔍 Analyzing network bottlenecks with exact MI...\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:Computing EXACT MI: torch.Size([48, 64]) → torch.Size([48, 64]) active neurons\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:Computing EXACT MI: torch.Size([48, 32]) → torch.Size([48, 18]) active neurons\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing integrated growth system...\n",
      "✅ Network created: 5 layers\n",
      "🔍 Testing network analysis...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'math'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n",
      "\u001b[32m     18\u001b[39m config.activation_threshold = \u001b[32m0.01\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m🔍 Testing network analysis...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m improved_network = \u001b[43manalyze_and_grow_network\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_growth_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_analysis_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n",
      "\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m✅ Analysis complete! Network type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(improved_network)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m🎯 Integrated growth system working!\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:638\u001b[39m, in \u001b[36manalyze_and_grow_network\u001b[39m\u001b[34m(network, train_loader, val_loader, threshold_config, max_growth_actions, num_analysis_batches)\u001b[39m\n",
      "\u001b[32m    635\u001b[39m growth_system = StructureNetGrowthSystem(network, threshold_config)\n",
      "\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# Analyze network\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m analysis_results = \u001b[43mgrowth_system\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_network_bottlenecks\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_analysis_batches\u001b[49m\n",
      "\u001b[32m    640\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    642\u001b[39m \u001b[38;5;66;03m# Apply improvements\u001b[39;00m\n",
      "\u001b[32m    643\u001b[39m improved_network = growth_system.apply_growth_recommendations(\n",
      "\u001b[32m    644\u001b[39m     analysis_results[\u001b[33m'\u001b[39m\u001b[33mrecommendations\u001b[39m\u001b[33m'\u001b[39m], max_growth_actions\n",
      "\u001b[32m    645\u001b[39m )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:337\u001b[39m, in \u001b[36mStructureNetGrowthSystem.analyze_network_bottlenecks\u001b[39m\u001b[34m(self, data_loader, num_batches)\u001b[39m\n",
      "\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# Analyze each layer pair\u001b[39;00m\n",
      "\u001b[32m    336\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sparse_layers) - \u001b[32m1\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     layer_pair_analysis = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_analyze_layer_pair\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\n",
      "\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    340\u001b[39m     analysis_results[\u001b[33m'\u001b[39m\u001b[33mlayer_analyses\u001b[39m\u001b[33m'\u001b[39m][\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] = layer_pair_analysis\n",
      "\u001b[32m    342\u001b[39m     \u001b[38;5;66;03m# Identify bottlenecks\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:415\u001b[39m, in \u001b[36mStructureNetGrowthSystem._analyze_layer_pair\u001b[39m\u001b[34m(self, layer_i, layer_j, data_loader, num_batches)\u001b[39m\n",
      "\u001b[32m    412\u001b[39m     acts_j = F.relu(acts_j)\n",
      "\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# Compute exact MI\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m mi_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexact_mi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_exact_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43macts_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macts_j\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# Compute dead neuron ratios\u001b[39;00m\n",
      "\u001b[32m    418\u001b[39m active_mask_i = (acts_i.abs() > \u001b[38;5;28mself\u001b[39m.threshold_config.activation_threshold).any(dim=\u001b[32m0\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:108\u001b[39m, in \u001b[36mExactMutualInformation.compute_exact_mi\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n",
      "\u001b[32m    105\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mexact_discrete\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[32m    106\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m n_active_X + n_active_Y <= \u001b[32m50\u001b[39m:\n",
      "\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# Use k-NN estimator (essentially exact for low dimensions)\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_knn_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_active\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_active\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    109\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mknn_exact\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[32m    110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# Still manageable with advanced estimators\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:191\u001b[39m, in \u001b[36mExactMutualInformation._knn_mi\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n",
      "\u001b[32m    188\u001b[39m mi = np.mean(mi_values)\n",
      "\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Estimate entropies using k-NN\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m h_x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_knn_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_np\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    192\u001b[39m h_y = \u001b[38;5;28mself\u001b[39m._knn_entropy(Y_np)\n",
      "\u001b[32m    193\u001b[39m h_xy = \u001b[38;5;28mself\u001b[39m._knn_entropy(np.hstack([X_np, Y_np]))\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/src/structure_net/evolution/integrated_growth_system.py:220\u001b[39m, in \u001b[36mExactMutualInformation._knn_entropy\u001b[39m\u001b[34m(self, X, k)\u001b[39m\n",
      "\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# Kozachenko-Leonenko estimator\u001b[39;00m\n",
      "\u001b[32m    219\u001b[39m d = X.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m volume = (np.pi ** (d/\u001b[32m2\u001b[39m)) / np.exp(np.log(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmath\u001b[49m.gamma(d/\u001b[32m2\u001b[39m + \u001b[32m1\u001b[39m)))\n",
      "\u001b[32m    222\u001b[39m h = np.log(rho + \u001b[32m1e-10\u001b[39m).mean() * d + np.log(volume) + np.log(X.shape[\u001b[32m0\u001b[39m]) - np.log(k)\n",
      "\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# Convert to bits\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/.pixi/envs/default/lib/python3.11/site-packages/numpy/__init__.py:795\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n",
      "\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchar\u001b[39;00m\n",
      "\u001b[32m    793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m char.chararray\n",
      "\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute 'math'"
     ]
    }
   ],
   "source": [
    "from src.structure_net import create_standard_network, analyze_and_grow_network, ThresholdConfig\n",
    "import torch\n",
    "\n",
    "print('🧪 Testing integrated growth system...')\n",
    "\n",
    "# Create a test network\n",
    "network = create_standard_network([784, 64, 32, 10], 0.02, device='cpu')\n",
    "print(f'✅ Network created: {len(network)} layers')\n",
    "\n",
    "# Create dummy data loader\n",
    "train_data = torch.randn(100, 784)\n",
    "train_labels = torch.randint(0, 10, (100,))\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "# Test the integrated growth system\n",
    "config = ThresholdConfig()\n",
    "config.activation_threshold = 0.01\n",
    "\n",
    "print('🔍 Testing network analysis...')\n",
    "improved_network = analyze_and_grow_network(\n",
    "    network, \n",
    "    train_loader, \n",
    "    threshold_config=config,\n",
    "    max_growth_actions=2,\n",
    "    num_analysis_batches=3\n",
    ")\n",
    "\n",
    "print(f'✅ Analysis complete! Network type: {type(improved_network)}')\n",
    "print('🎯 Integrated growth system working!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b6131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.structure_net.evolution.integrated_growth_system:🔍 Analyzing network bottlenecks with exact MI...\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:Computing EXACT MI: torch.Size([48, 64]) → torch.Size([48, 64]) active neurons\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:Computing EXACT MI: torch.Size([48, 32]) → torch.Size([48, 19]) active neurons\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:\n",
      "📊 NETWORK ANALYSIS SUMMARY\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:==================================================\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:\n",
      "🚨 Found 2 information bottlenecks:\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:   Layer 0→1: MI efficiency -0.09% (severity: 1.00)\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:   Layer 1→2: MI efficiency -0.02% (severity: 1.00)\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:\n",
      "✅ No significant dead zones found\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:\n",
      "💡 Generated 2 recommendations:\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:   1. insert_layer: Severe information bottleneck (efficiency: -0.09%) (priority: high)\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:   2. insert_layer: Severe information bottleneck (efficiency: -0.02%) (priority: high)\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:🌱 Applying top 2 growth recommendations...\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:   1. insert_layer: Severe information bottleneck (efficiency: -0.09%)\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:      Inserted layer of width 268 at position 1\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:   2. insert_layer: Severe information bottleneck (efficiency: -0.02%)\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:      Inserted layer of width 157 at position 2\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:   📐 New architecture: [784, 268, 157, 64, 32, 10]\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:   🔄 Transferring weights from old network...\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:      Layer 0 has new dimensions - using random initialization\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:      Layer 1 has new dimensions - using random initialization\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:      Layer 2 has new dimensions - using random initialization\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:      Layer 3 has new dimensions - using random initialization\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:      Copied weights for layer 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing integrated growth system...\n",
      "✅ Network created: 5 layers\n",
      "🔍 Testing network analysis...\n",
      "✅ Analysis complete! Network type: <class 'torch.nn.modules.container.Sequential'>\n",
      "🎯 Integrated growth system working!\n"
     ]
    }
   ],
   "source": [
    "from src.structure_net import create_standard_network, analyze_and_grow_network, ThresholdConfig\n",
    "import torch\n",
    "\n",
    "print('🧪 Testing integrated growth system...')\n",
    "\n",
    "# Create a test network\n",
    "network = create_standard_network([784, 64, 32, 10], 0.02, device='cpu')\n",
    "print(f'✅ Network created: {len(network)} layers')\n",
    "\n",
    "# Create dummy data loader\n",
    "train_data = torch.randn(100, 784)\n",
    "train_labels = torch.randint(0, 10, (100,))\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "# Test the integrated growth system\n",
    "config = ThresholdConfig()\n",
    "config.activation_threshold = 0.01\n",
    "\n",
    "print('🔍 Testing network analysis...')\n",
    "improved_network = analyze_and_grow_network(\n",
    "    network, \n",
    "    train_loader, \n",
    "    threshold_config=config,\n",
    "    max_growth_actions=2,\n",
    "    num_analysis_batches=3\n",
    ")\n",
    "\n",
    "print(f'✅ Analysis complete! Network type: {type(improved_network)}')\n",
    "print('🎯 Integrated growth system working!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab1f600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Direct import successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from src.structure_net.evolution.integrated_growth_system import analyze_and_grow_network\n",
    "    print('✅ Direct import successful')\n",
    "except Exception as e:\n",
    "    print(f'❌ Direct import failed: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b47a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.structure_net.evolution.complete_metrics_system:🔬 Computing complete metrics suite...\n",
      "INFO:src.structure_net.evolution.complete_metrics_system:  Computing MI metrics...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing complete metrics system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.structure_net.evolution.complete_metrics_system:  Computing activity metrics...\n",
      "INFO:src.structure_net.evolution.complete_metrics_system:  Computing SensLI metrics...\n",
      "INFO:src.structure_net.evolution.complete_metrics_system:  Computing graph metrics...\n",
      "WARNING:src.structure_net.evolution.complete_metrics_system:Spectral computation failed: ARPACK error -1: No convergence (1381 iterations, 0/10 eigenvectors converged)\n",
      "INFO:src.structure_net.evolution.complete_metrics_system:✅ Complete metrics computation finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Complete metrics computed!\n",
      "   MI metrics: 2 layer pairs\n",
      "   Activity metrics: 3 layers\n",
      "   SensLI metrics: 2 layer pairs\n",
      "   Graph built: True\n",
      "   Summary: 11 high-level metrics\n",
      "🎯 Complete metrics system working!\n"
     ]
    }
   ],
   "source": [
    "from src.structure_net.evolution.complete_metrics_system import CompleteMetricsSystem\n",
    "from src.structure_net import create_standard_network, ThresholdConfig, MetricsConfig\n",
    "import torch\n",
    "\n",
    "print('🧪 Testing complete metrics system...')\n",
    "\n",
    "# Create network and data\n",
    "network = create_standard_network([784, 128, 64, 10], 0.02, device='cpu')\n",
    "train_data = torch.randn(100, 784)\n",
    "train_labels = torch.randint(0, 10, (100,))\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "# Create configs\n",
    "threshold_config = ThresholdConfig()\n",
    "metrics_config = MetricsConfig()\n",
    "\n",
    "# Test complete system\n",
    "complete_system = CompleteMetricsSystem(network, threshold_config, metrics_config)\n",
    "all_metrics = complete_system.compute_all_metrics(train_loader, num_batches=3)\n",
    "\n",
    "print(f'✅ Complete metrics computed!')\n",
    "print(f'   MI metrics: {len(all_metrics[\"mi_metrics\"])} layer pairs')\n",
    "print(f'   Activity metrics: {len(all_metrics[\"activity_metrics\"])} layers')\n",
    "print(f'   SensLI metrics: {len(all_metrics[\"sensli_metrics\"])} layer pairs')\n",
    "print(f'   Graph built: {all_metrics[\"graph_metrics\"][\"graph_built\"]}')\n",
    "print(f'   Summary: {len(all_metrics[\"summary\"])} high-level metrics')\n",
    "print('🎯 Complete metrics system working!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55286260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.structure_net.evolution.complete_metrics_system:🔬 Computing complete metrics suite...\n",
      "INFO:src.structure_net.evolution.complete_metrics_system:  Computing MI metrics...\n",
      "INFO:src.structure_net.evolution.complete_metrics_system:  Computing activity metrics...\n",
      "INFO:src.structure_net.evolution.complete_metrics_system:  Computing SensLI metrics...\n",
      "INFO:src.structure_net.evolution.complete_metrics_system:  Computing graph metrics...\n",
      "INFO:src.structure_net.evolution.complete_metrics_system:✅ Complete metrics computation finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimized metrics system...\n",
      "✅ Optimized metrics system works correctly!\n",
      "   MI metrics computed: 1 layer pairs\n",
      "   Activity metrics computed: 2 layers\n",
      "   SensLI metrics computed: 1 layer pairs\n",
      "   Graph metrics computed: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.structure_net.core.network_factory import create_standard_network\n",
    "from src.structure_net.evolution.complete_metrics_system import CompleteMetricsSystem\n",
    "from src.structure_net.evolution.integrated_growth_system import ThresholdConfig, MetricsConfig\n",
    "\n",
    "# Create a simple test network\n",
    "network = create_standard_network([10, 5, 2], sparsity=0.1, device='cpu')\n",
    "\n",
    "# Create configs\n",
    "threshold_config = ThresholdConfig()\n",
    "metrics_config = MetricsConfig()\n",
    "\n",
    "# Create metrics system\n",
    "metrics_system = CompleteMetricsSystem(network, threshold_config, metrics_config)\n",
    "\n",
    "# Create dummy data\n",
    "data = torch.randn(32, 10)\n",
    "target = torch.randint(0, 2, (32,))\n",
    "dummy_loader = [(data, target)]\n",
    "\n",
    "print('Testing optimized metrics system...')\n",
    "try:\n",
    "    results = metrics_system.compute_all_metrics(dummy_loader, num_batches=1)\n",
    "    print('✅ Optimized metrics system works correctly!')\n",
    "    print(f'   MI metrics computed: {len(results[\"mi_metrics\"])} layer pairs')\n",
    "    print(f'   Activity metrics computed: {len(results[\"activity_metrics\"])} layers')\n",
    "    print(f'   SensLI metrics computed: {len(results[\"sensli_metrics\"])} layer pairs')\n",
    "    print(f'   Graph metrics computed: {\"graph_built\" in results[\"graph_metrics\"]}')\n",
    "    \n",
    "    # Check if optimization flag is present\n",
    "    for key, sensli_result in results['sensli_metrics'].items():\n",
    "        if 'optimization' in sensli_result:\n",
    "            print(f'   ✅ SensLI optimization confirmed: {sensli_result[\"optimization\"]}')\n",
    "            break\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'❌ Error: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e48e52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'IntegratedGrowthSystem' from 'src.structure_net.evolution.integrated_growth_system' (/home/rabbit/structure_net/src/structure_net/evolution/integrated_growth_system.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstructure_net\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnetwork_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_standard_network\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstructure_net\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevolution\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrated_growth_system\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntegratedGrowthSystem, ThresholdConfig, MetricsConfig\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTesting enhanced tournament system with advanced strategies...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Create a simple test network\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'IntegratedGrowthSystem' from 'src.structure_net.evolution.integrated_growth_system' (/home/rabbit/structure_net/src/structure_net/evolution/integrated_growth_system.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.structure_net.core.network_factory import create_standard_network\n",
    "from src.structure_net.evolution.integrated_growth_system import IntegratedGrowthSystem, ThresholdConfig, MetricsConfig\n",
    "\n",
    "print('Testing enhanced tournament system with advanced strategies...')\n",
    "try:\n",
    "    # Create a simple test network\n",
    "    network = create_standard_network([10, 8, 5, 2], sparsity=0.1, device='cpu')\n",
    "    \n",
    "    # Create configs\n",
    "    threshold_config = ThresholdConfig()\n",
    "    metrics_config = MetricsConfig()\n",
    "    \n",
    "    # Create integrated system\n",
    "    system = IntegratedGrowthSystem(network, threshold_config, metrics_config)\n",
    "    \n",
    "    # Create dummy data\n",
    "    data = torch.randn(32, 10)\n",
    "    target = torch.randint(0, 2, (32,))\n",
    "    dummy_loader = [(data, target)]\n",
    "    \n",
    "    # Test tournament with advanced strategies\n",
    "    tournament_results = system.tournament.run_tournament(dummy_loader, dummy_loader, growth_iterations=1, epochs_per_iteration=1)\n",
    "    \n",
    "    print('✅ Enhanced tournament system works correctly!')\n",
    "    print(f'   Winner: {tournament_results[\"winner\"][\"strategy\"]}')\n",
    "    print(f'   Improvement: {tournament_results[\"winner\"][\"improvement\"]:.2%}')\n",
    "    print(f'   Actions: {tournament_results[\"winner\"][\"actions\"]}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'❌ Error: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee34ac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.structure_net.evolution.complete_metrics_system:\n",
      "🔄 METRICS SYSTEM MIGRATION NOTICE:\n",
      "\n",
      "The complete metrics system has been refactored into modular components for better maintainability.\n",
      "\n",
      "NEW RECOMMENDED IMPORTS:\n",
      "  from structure_net.evolution.metrics import CompleteMetricsSystem\n",
      "  from structure_net.evolution.metrics import MutualInformationAnalyzer, ActivityAnalyzer\n",
      "  from structure_net.evolution.autocorrelation import PerformanceAnalyzer\n",
      "\n",
      "OLD IMPORTS (still work but deprecated):\n",
      "  from structure_net.evolution.complete_metrics_system import CompleteMetricsSystem\n",
      "\n",
      "The new modular system provides:\n",
      "✅ Better performance through optimized data collection\n",
      "✅ Enhanced autocorrelation framework for meta-learning\n",
      "✅ Improved caching and computation statistics\n",
      "✅ Cleaner separation of concerns\n",
      "✅ Better testing and maintainability\n",
      "\n",
      "Your existing code will continue to work without changes.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n",
      "Available classes:\n",
      "- BaseLearningRateScheduler\n",
      "- LearningRateStrategy\n",
      "- ExponentialBackoffScheduler\n",
      "- LayerwiseAdaptiveRates\n",
      "- create_adaptive_manager\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from src.structure_net.evolution.adaptive_learning_rates import (\n",
    "        BaseLearningRateScheduler,\n",
    "        LearningRateStrategy,\n",
    "        ExponentialBackoffScheduler,\n",
    "        LayerwiseAdaptiveRates,\n",
    "        create_adaptive_manager\n",
    "    )\n",
    "    print('✅ All imports successful!')\n",
    "    print('Available classes:')\n",
    "    print('- BaseLearningRateScheduler')\n",
    "    print('- LearningRateStrategy')\n",
    "    print('- ExponentialBackoffScheduler')\n",
    "    print('- LayerwiseAdaptiveRates')\n",
    "    print('- create_adaptive_manager')\n",
    "except ImportError as e:\n",
    "    print(f'❌ Import error: {e}')\n",
    "except Exception as e:\n",
    "    print(f'❌ Other error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbf789f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All advanced imports successful!\n",
      "📦 Available modules:\n",
      "  Phase Schedulers: ExtremaPhaseScheduler, GrowthPhaseScheduler, WarmupScheduler\n",
      "  Layer Schedulers: CascadingDecayScheduler, ProgressiveFreezingScheduler, SedimentaryLearningScheduler\n",
      "  Connection Schedulers: MultiScaleLearning, SoftClampingScheduler, SparsityAwareScheduler\n",
      "  Unified System: AdaptiveLearningRateManager, UnifiedAdaptiveLearning\n",
      "  Factory Functions: create_*_manager, create_scheduler_presets\n",
      "📋 Available presets: ['conservative', 'aggressive', 'balanced', 'extrema_focused', 'fine_tuning']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from src.structure_net.evolution.adaptive_learning_rates import (\n",
    "        # Phase schedulers\n",
    "        ExtremaPhaseScheduler,\n",
    "        GrowthPhaseScheduler,\n",
    "        WarmupScheduler,\n",
    "        # Layer schedulers  \n",
    "        CascadingDecayScheduler,\n",
    "        ProgressiveFreezingScheduler,\n",
    "        SedimentaryLearningScheduler,\n",
    "        # Connection schedulers\n",
    "        MultiScaleLearning,\n",
    "        SoftClampingScheduler,\n",
    "        SparsityAwareScheduler,\n",
    "        # Unified system\n",
    "        AdaptiveLearningRateManager,\n",
    "        UnifiedAdaptiveLearning,\n",
    "        # Factory functions\n",
    "        create_basic_manager,\n",
    "        create_advanced_manager,\n",
    "        create_comprehensive_manager,\n",
    "        create_ultimate_manager,\n",
    "        create_preset_manager,\n",
    "        create_scheduler_presets\n",
    "    )\n",
    "    print('✅ All advanced imports successful!')\n",
    "    print('📦 Available modules:')\n",
    "    print('  Phase Schedulers: ExtremaPhaseScheduler, GrowthPhaseScheduler, WarmupScheduler')\n",
    "    print('  Layer Schedulers: CascadingDecayScheduler, ProgressiveFreezingScheduler, SedimentaryLearningScheduler')\n",
    "    print('  Connection Schedulers: MultiScaleLearning, SoftClampingScheduler, SparsityAwareScheduler')\n",
    "    print('  Unified System: AdaptiveLearningRateManager, UnifiedAdaptiveLearning')\n",
    "    print('  Factory Functions: create_*_manager, create_scheduler_presets')\n",
    "    \n",
    "    # Test preset creation\n",
    "    presets = create_scheduler_presets()\n",
    "    print(f'📋 Available presets: {list(presets.keys())}')\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f'❌ Import error: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "except Exception as e:\n",
    "    print(f'❌ Other error: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843be01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Testing profiling system...\n",
      "🔬 ProfilerManager initialized (level: BASIC)\n",
      "🧬 EvolutionProfiler initialized\n",
      "📊 Registered profiler: evolution_profiler\n",
      "🪶 Created lightweight profiler (minimal overhead)\n",
      "🚀 Started profiling session: test_session\n",
      "💾 Session results saved to profiling_results/session_test_session.json\n",
      "📊 Profiling results saved to profiling_results/evolution_profiler_profile_1752021867.json\n",
      "🏁 Ended profiling session: test_session (duration: 0.02s)\n",
      "✅ Profiling system working correctly!\n",
      "📊 Tracked 1 operations\n"
     ]
    }
   ],
   "source": [
    from src.profiling import create_lightweight_profiler, profile_function

    "import time\n",
    "\n",
    "# Test basic profiling functionality\n",
    "print('🔬 Testing profiling system...')\n",
    "\n",
    "# Create profiler\n",
    "profiler = create_lightweight_profiler()\n",
    "profiler.start_session('test_session')\n",
    "\n",
    "# Test decorator\n",
    "@profile_function(component='test')\n",
    "def test_function():\n",
    "    time.sleep(0.01)\n",
    "    return 42\n",
    "\n",
    "# Test context manager\n",
    "with profiler.profile_operation('test_operation', 'test') as ctx:\n",
    "    ctx.add_metric('test_metric', 123)\n",
    "    time.sleep(0.01)\n",
    "\n",
    "# Run test function\n",
    "result = test_function()\n",
    "\n",
    "# Get results\n",
    "session_results = profiler.end_session()\n",
    "print('✅ Profiling system working correctly!')\n",
    "print(f'📊 Tracked {len(profiler.get_aggregated_metrics().get(\"operation_breakdown\", {}))} operations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da983fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.structure_net.evolution.complete_metrics_system:\n",
      "🔄 METRICS SYSTEM MIGRATION NOTICE:\n",
      "\n",
      "The complete metrics system has been refactored into modular components for better maintainability.\n",
      "\n",
      "NEW RECOMMENDED IMPORTS:\n",
      "  from structure_net.evolution.metrics import CompleteMetricsSystem\n",
      "  from structure_net.evolution.metrics import MutualInformationAnalyzer, ActivityAnalyzer\n",
      "  from structure_net.evolution.autocorrelation import PerformanceAnalyzer\n",
      "\n",
      "OLD IMPORTS (still work but deprecated):\n",
      "  from structure_net.evolution.complete_metrics_system import CompleteMetricsSystem\n",
      "\n",
      "The new modular system provides:\n",
      "✅ Better performance through optimized data collection\n",
      "✅ Enhanced autocorrelation framework for meta-learning\n",
      "✅ Improved caching and computation statistics\n",
      "✅ Cleaner separation of concerns\n",
      "✅ Better testing and maintainability\n",
      "\n",
      "Your existing code will continue to work without changes.\n",
      "\n",
      "INFO:src.structure_net.evolution.integrated_growth_system_v2:\n",
      "🔄 INTEGRATED GROWTH SYSTEM MIGRATION\n",
      "\n",
      "This module now uses the new composable evolution system as its backend.\n",
      "Your existing code will continue to work without changes.\n",
      "\n",
      "MIGRATION BENEFITS:\n",
      "✅ Better performance through optimized components\n",
      "✅ Modular architecture for easier customization  \n",
      "✅ Enhanced monitoring and debugging capabilities\n",
      "✅ Future-proof design for new research directions\n",
      "\n",
      "RECOMMENDED MIGRATION:\n",
      "  OLD: from structure_net.evolution.integrated_growth_system import IntegratedGrowthSystem\n",
      "  NEW: from structure_net.evolution.components import create_standard_evolution_system\n",
      "\n",
      "See docs/composable_evolution_guide.md for complete migration guide.\n",
      "\n",
      "INFO:src.structure_net.evolution.integrated_growth_system:\n",
      "🔄 INTEGRATED GROWTH SYSTEM MIGRATION NOTICE\n",
      "\n",
      "This module now uses the new composable evolution architecture as its backend.\n",
      "Your existing code will continue to work without changes.\n",
      "\n",
      "WHAT CHANGED:\n",
      "✅ Same API, better performance\n",
      "✅ Modular architecture under the hood\n",
      "✅ Enhanced monitoring and debugging\n",
      "✅ Future-proof design\n",
      "\n",
      "MIGRATION OPTIONS:\n",
      "1. NO CHANGES NEEDED - Your code works automatically with new backend\n",
      "2. GRADUAL MIGRATION - Use new composable API for new features\n",
      "3. FULL MIGRATION - Migrate to composable system for maximum benefits\n",
      "\n",
      "RESOURCES:\n",
      "📖 Migration Guide: docs/integrated_growth_system_migration.md\n",
      "🔧 Migration Helper: from structure_net.evolution.migration_helper import MigrationHelper\n",
      "📝 Examples: examples/composable_evolution_example.py\n",
      "\n",
      "Your code continues to work exactly as before, now with improved performance!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful\n"
     ]
    }
   ],
   "source": [
    "from src.structure_net.core.layers import StandardSparseLayer;\n",
    "print('Import successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b274308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES already set to: 1,2\n",
      "CUDA_VISIBLE_DEVICES already set to: 1,2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rabbit/structure_net/.pixi/envs/default/lib/python3.11/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'validate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES already set to: 1,2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_factory.config:Registered dataset: mnist (MNIST Handwritten Digits)\n",
      "INFO:data_factory.config:Registered dataset: cifar10 (CIFAR-10)\n",
      "INFO:data_factory.config:Registered dataset: cifar100 (CIFAR-100)\n",
      "INFO:data_factory.config:Registered dataset: imagenet (ImageNet)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES already set to: 1,2\n",
      "Import successful\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.insert(0, '.'); from src.neural_architecture_lab.core import LabConfig; print('Import successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54677ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_factory imports successfully\n"
     ]
    }
   ],
   "source": [
    "from data_factory import create_dataset \n",
    "print('data_factory imports successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79f8042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful\n"
     ]
    }
   ],
   "source": [
    "from src.neural_architecture_lab import NeuralArchitectureLab; print('Import successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69792504",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m; \n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m d = datasets.CIFAR10(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m, train=\u001b[38;5;28;01mTrue\u001b[39;00m, download=\u001b[38;5;28;01mFalse\u001b[39;00m, transform=transforms.ToTensor()); \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCIFAR10 shape:          │ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structure_net/.pixi/envs/default/lib/python3.11/site-packages/torchvision/datasets/cifar.py:69\u001b[39m, in \u001b[36mCIFAR10.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mself\u001b[39m.download()\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train:\n\u001b[32m     72\u001b[39m     downloaded_list = \u001b[38;5;28mself\u001b[39m.train_list\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "import torch; \n",
    "from torchvision import datasets, transforms\n",
    "d = datasets.CIFAR10('.', train=True, download=False, transform=transforms.ToTensor()); print(f'CIFAR10 shape:          │ {d[0][0].shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
