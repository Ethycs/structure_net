{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d94d81",
   "metadata": {},
   "source": [
    "# 🧠 NeuroFlow Demo: Catastrophe Detection via Neural Navier-Stokes\n",
    "This notebook demonstrates the integration of **Neural Flow diagnostics** with a small network trained on synthetic data designed to induce decision boundary singularities (catastrophes).\n",
    "\n",
    "We will:\n",
    "- Build a synthetic triple-point dataset\n",
    "- Train a simple neural net\n",
    "- Compute chain map homology (zero zones)\n",
    "- Estimate boundary vorticity (turbulence)\n",
    "- Visualize predicted singularity regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b7462",
   "metadata": {},
   "source": [
    "## 📊 Step 1: Generate Triple-Point Data\n",
    "This creates three classes that converge at a central point, forcing decision boundaries into contact — ideal for studying dynamic instabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e170b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def generate_triple_point_data(n=1500, radius=2.0, noise=0.1):\n",
    "    X, y = [], []\n",
    "    for i in range(n):\n",
    "        angle = np.random.uniform(0, 2*np.pi)\n",
    "        r = np.random.normal(loc=radius, scale=noise)\n",
    "        x = r * np.cos(angle)\n",
    "        y_coord = r * np.sin(angle)\n",
    "        sector = int((angle % (2*np.pi)) // (2*np.pi/3))\n",
    "        X.append([x, y_coord])\n",
    "        y.append(sector)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate and plot\n",
    "X, y = generate_triple_point_data()\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='Accent'); plt.title('Triple Point Data'); plt.axis('equal'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ade858",
   "metadata": {},
   "source": [
    "## 🧠 Step 2: Define a Simple Neural Network\n",
    "We'll use a compact 2-layer MLP that learns this decision surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30487c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.out = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# Prepare data loaders\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = SimpleNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990aaa6",
   "metadata": {},
   "source": [
    "## 💾 Step 3: Load Pretrained Weights (Optional)\n",
    "You can load any saved PyTorch model weights here to analyze a trained network on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09834fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to a saved model checkpoint (.pt or .pth)\n",
    "import os\n",
    "model_path = '/mnt/data/your_model_weights.pth'  # <- Change this!\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    state_dict = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    print('✅ Loaded model weights successfully.')\n",
    "else:\n",
    "    print('⚠️ Model file not found. Please upload a .pth file to /mnt/data and set model_path.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b91493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model accuracy on training data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in dataloader:\n",
    "        logits = model(xb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "print(f\"Training Accuracy: {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175bc79",
   "metadata": {},
   "source": [
    "## 🌊 Step 4: Estimate Flow Field via Dynamic Mode Decomposition (DMD)\n",
    "We'll simulate how the decision boundary 'moves' during training by collecting activations over epochs and applying DMD to estimate the temporal evolution of internal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a345ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Cache activations from intermediate layer across epochs\n",
    "activation_history = deque(maxlen=10)\n",
    "\n",
    "# Modify model to store activations\n",
    "activation_cache = {}\n",
    "\n",
    "def cache_hook(module, input, output):\n",
    "    activation_cache['x'] = output.detach()\n",
    "\n",
    "hook = model.fc2.register_forward_hook(cache_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for multiple fake epochs to collect data (or do this during real training)\n",
    "for i in range(10):\n",
    "    model.eval()\n",
    "    for xb, yb in dataloader:\n",
    "        _ = model(xb)\n",
    "        break  # Just grab one batch\n",
    "    act = activation_cache['x'].flatten(start_dim=1)\n",
    "    activation_history.append(act)\n",
    "\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMD estimation: compute modes from activation snapshots\n",
    "from numpy.linalg import svd\n",
    "\n",
    "X1 = torch.stack(list(activation_history)[:-1]).numpy().transpose(1, 0, 2).reshape(-1, 9)\n",
    "X2 = torch.stack(list(activation_history)[1:]).numpy().transpose(1, 0, 2).reshape(-1, 9)\n",
    "\n",
    "U, S, Vh = svd(X1, full_matrices=False)\n",
    "rank = 5\n",
    "Ur = U[:, :rank]\n",
    "Sr = np.diag(S[:rank])\n",
    "Vr = Vh[:rank, :]\n",
    "A_tilde = Ur.T @ X2 @ Vr.T @ np.linalg.inv(Sr)\n",
    "\n",
    "# Eigen decomposition\n",
    "evals, evecs = np.linalg.eig(A_tilde)\n",
    "Phi = X2 @ Vr.T @ np.linalg.inv(Sr) @ evecs  # DMD modes\n",
    "\n",
    "print(\"✅ DMD completed. Eigenvalues:\", np.round(evals, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a274942",
   "metadata": {},
   "source": [
    "## 🌀 Step 5: Estimate Vorticity (Curl) of the Flow Field\n",
    "We'll compute a proxy for vorticity by estimating the local rotational energy in the activation evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate finite difference of flow vectors\n",
    "V = X2 - X1\n",
    "flow_magnitude = np.linalg.norm(V, axis=0)\n",
    "plt.plot(flow_magnitude)\n",
    "plt.title('Estimated Flow Magnitude per Activation Channel'); plt.xlabel('Channel'); plt.ylabel('||v||'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba2df5",
   "metadata": {},
   "source": [
    "## 🧮 Step 6: Chain Map Homology (Zero Map Detection)\n",
    "We'll compute the null space (kernel) of weight matrices using SVD to detect structural 'dead zones' in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homology_loss(weight_matrix, threshold=1e-4):\n",
    "    U, S, Vh = torch.linalg.svd(weight_matrix, full_matrices=False)\n",
    "    small_svs = S[S < threshold]\n",
    "    return len(small_svs), small_svs.cpu().numpy()\n",
    "\n",
    "n_null, null_vals = homology_loss(model.fc2.weight)\n",
    "print(f\"🧩 fc2 kernel dimension: {n_null}\\nSingular values: {null_vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb95132",
   "metadata": {},
   "source": [
    "## 🔥 Step 7: Detect Catastrophes as Kernel ∩ Vorticity Regions\n",
    "Theoretically, singularities emerge where the boundary flow is squeezed by the network's structural constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple proxy: plot both kernel size and flow magnitude to detect overlap\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Activation Index')\n",
    "ax1.set_ylabel('Flow ||v||', color=color)\n",
    "ax1.plot(flow_magnitude, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('# Near-Zero SVs', color=color)\n",
    "ax2.axhline(n_null, color=color, linestyle='--', label='Kernel Size')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "plt.title('Catastrophe Detection: Kernel vs. Vorticity')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4638b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model layers:\n",
      "  conv1: Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  conv2: Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  fc1: Linear(in_features=9216, out_features=128, bias=True)\n",
      "  fc2: Linear(in_features=128, out_features=10, bias=True)\n",
      "🚀 Starting training with NeuroFlowEngine on MNIST...\n",
      "  Epoch 1/5 | Batch 0/937 | Task Loss: 2.3034 | NeuroFlow Loss: 0.0102\n",
      "  Epoch 1/5 | Batch 100/937 | Task Loss: 0.3751 | NeuroFlow Loss: 0.0212\n",
      "  Epoch 1/5 | Batch 200/937 | Task Loss: 0.2043 | NeuroFlow Loss: 0.0217\n",
      "  Epoch 1/5 | Batch 300/937 | Task Loss: 0.0770 | NeuroFlow Loss: 0.0199\n",
      "  Epoch 1/5 | Batch 400/937 | Task Loss: 0.2602 | NeuroFlow Loss: 0.0193\n",
      "  Epoch 1/5 | Batch 500/937 | Task Loss: 0.1940 | NeuroFlow Loss: 0.0189\n",
      "  Epoch 1/5 | Batch 600/937 | Task Loss: 0.0913 | NeuroFlow Loss: 0.0192\n",
      "  Epoch 1/5 | Batch 700/937 | Task Loss: 0.0919 | NeuroFlow Loss: 0.0193\n",
      "  Epoch 1/5 | Batch 800/937 | Task Loss: 0.0362 | NeuroFlow Loss: 0.0180\n",
      "  Epoch 1/5 | Batch 900/937 | Task Loss: 0.1500 | NeuroFlow Loss: 0.0186\n",
      "------------------------------------------------------------\n",
      "✅ Epoch 1 Complete\n",
      "   Avg Task Loss: 0.1979\n",
      "   Avg NeuroFlow Loss: 0.0198\n",
      "   Accuracy: 93.97%\n",
      "------------------------------------------------------------\n",
      "  Epoch 2/5 | Batch 0/937 | Task Loss: 0.1274 | NeuroFlow Loss: 0.0119\n",
      "  Epoch 2/5 | Batch 100/937 | Task Loss: 0.0636 | NeuroFlow Loss: 0.0184\n",
      "  Epoch 2/5 | Batch 200/937 | Task Loss: 0.1099 | NeuroFlow Loss: 0.0196\n",
      "  Epoch 2/5 | Batch 300/937 | Task Loss: 0.2546 | NeuroFlow Loss: 0.0185\n",
      "  Epoch 2/5 | Batch 400/937 | Task Loss: 0.0347 | NeuroFlow Loss: 0.0184\n",
      "  Epoch 2/5 | Batch 500/937 | Task Loss: 0.2219 | NeuroFlow Loss: 0.0173\n",
      "  Epoch 2/5 | Batch 600/937 | Task Loss: 0.0189 | NeuroFlow Loss: 0.0177\n",
      "  Epoch 2/5 | Batch 700/937 | Task Loss: 0.0689 | NeuroFlow Loss: 0.0185\n",
      "  Epoch 2/5 | Batch 800/937 | Task Loss: 0.0761 | NeuroFlow Loss: 0.0173\n",
      "  Epoch 2/5 | Batch 900/937 | Task Loss: 0.1114 | NeuroFlow Loss: 0.0174\n",
      "------------------------------------------------------------\n",
      "✅ Epoch 2 Complete\n",
      "   Avg Task Loss: 0.0811\n",
      "   Avg NeuroFlow Loss: 0.0180\n",
      "   Accuracy: 97.53%\n",
      "------------------------------------------------------------\n",
      "  Epoch 3/5 | Batch 0/937 | Task Loss: 0.0054 | NeuroFlow Loss: 0.0116\n",
      "  Epoch 3/5 | Batch 100/937 | Task Loss: 0.1033 | NeuroFlow Loss: 0.0168\n",
      "  Epoch 3/5 | Batch 200/937 | Task Loss: 0.0691 | NeuroFlow Loss: 0.0170\n",
      "  Epoch 3/5 | Batch 300/937 | Task Loss: 0.0588 | NeuroFlow Loss: 0.0172\n",
      "  Epoch 3/5 | Batch 400/937 | Task Loss: 0.0976 | NeuroFlow Loss: 0.0169\n",
      "  Epoch 3/5 | Batch 500/937 | Task Loss: 0.0399 | NeuroFlow Loss: 0.0166\n",
      "  Epoch 3/5 | Batch 600/937 | Task Loss: 0.0402 | NeuroFlow Loss: 0.0164\n",
      "  Epoch 3/5 | Batch 700/937 | Task Loss: 0.0906 | NeuroFlow Loss: 0.0169\n",
      "  Epoch 3/5 | Batch 800/937 | Task Loss: 0.1668 | NeuroFlow Loss: 0.0169\n",
      "  Epoch 3/5 | Batch 900/937 | Task Loss: 0.0164 | NeuroFlow Loss: 0.0154\n",
      "------------------------------------------------------------\n",
      "✅ Epoch 3 Complete\n",
      "   Avg Task Loss: 0.0642\n",
      "   Avg NeuroFlow Loss: 0.0168\n",
      "   Accuracy: 97.96%\n",
      "------------------------------------------------------------\n",
      "  Epoch 4/5 | Batch 0/937 | Task Loss: 0.0164 | NeuroFlow Loss: 0.0112\n",
      "  Epoch 4/5 | Batch 100/937 | Task Loss: 0.0347 | NeuroFlow Loss: 0.0162\n",
      "  Epoch 4/5 | Batch 200/937 | Task Loss: 0.0457 | NeuroFlow Loss: 0.0156\n",
      "  Epoch 4/5 | Batch 300/937 | Task Loss: 0.0167 | NeuroFlow Loss: 0.0158\n",
      "  Epoch 4/5 | Batch 400/937 | Task Loss: 0.0363 | NeuroFlow Loss: 0.0155\n",
      "  Epoch 4/5 | Batch 500/937 | Task Loss: 0.0165 | NeuroFlow Loss: 0.0157\n",
      "  Epoch 4/5 | Batch 600/937 | Task Loss: 0.0192 | NeuroFlow Loss: 0.0155\n",
      "  Epoch 4/5 | Batch 700/937 | Task Loss: 0.0107 | NeuroFlow Loss: 0.0151\n",
      "  Epoch 4/5 | Batch 800/937 | Task Loss: 0.0203 | NeuroFlow Loss: 0.0154\n",
      "  Epoch 4/5 | Batch 900/937 | Task Loss: 0.0564 | NeuroFlow Loss: 0.0151\n",
      "------------------------------------------------------------\n",
      "✅ Epoch 4 Complete\n",
      "   Avg Task Loss: 0.0510\n",
      "   Avg NeuroFlow Loss: 0.0157\n",
      "   Accuracy: 98.43%\n",
      "------------------------------------------------------------\n",
      "  Epoch 5/5 | Batch 0/937 | Task Loss: 0.0497 | NeuroFlow Loss: 0.0108\n",
      "  Epoch 5/5 | Batch 100/937 | Task Loss: 0.0578 | NeuroFlow Loss: 0.0149\n",
      "  Epoch 5/5 | Batch 200/937 | Task Loss: 0.0091 | NeuroFlow Loss: 0.0148\n",
      "  Epoch 5/5 | Batch 300/937 | Task Loss: 0.0647 | NeuroFlow Loss: 0.0150\n",
      "  Epoch 5/5 | Batch 400/937 | Task Loss: 0.0033 | NeuroFlow Loss: 0.0149\n",
      "  Epoch 5/5 | Batch 500/937 | Task Loss: 0.0056 | NeuroFlow Loss: 0.0147\n",
      "  Epoch 5/5 | Batch 600/937 | Task Loss: 0.0142 | NeuroFlow Loss: 0.0144\n",
      "  Epoch 5/5 | Batch 700/937 | Task Loss: 0.0142 | NeuroFlow Loss: 0.0147\n",
      "  Epoch 5/5 | Batch 800/937 | Task Loss: 0.1240 | NeuroFlow Loss: 0.0142\n",
      "  Epoch 5/5 | Batch 900/937 | Task Loss: 0.1661 | NeuroFlow Loss: 0.0145\n",
      "------------------------------------------------------------\n",
      "✅ Epoch 5 Complete\n",
      "   Avg Task Loss: 0.0450\n",
      "   Avg NeuroFlow Loss: 0.0148\n",
      "   Accuracy: 98.53%\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔬 Performing final analysis on the trained model...\n",
      "\n",
      "--- Final NeuroFlow Diagnostics ---\n",
      "Homology Loss: 1.0455\n",
      "  - Dead Zones per Layer: {'conv1': 0.0, 'conv2': 0.0}\n",
      "DMD (Vorticity) Loss: 0.0794\n",
      "GFT (Spectral) Loss: 3.7876\n",
      "LNN Residual Norm (proxy): 0.3788\n",
      "Catastrophe Map (High-Residual Nodes): tensor([], device='cuda:0', dtype=torch.int64)\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# --- PyTorch Profiling (Optional but Recommended) ---\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===================================================================\n",
    "# 🧠 SYSTEM: NeuroFlowEngine\n",
    "# ===================================================================\n",
    "\n",
    "class NeuroFlowEngine(nn.Module):\n",
    "    \"\"\"\n",
    "    The main orchestrator for the NeuroFlow diagnostic and control system.\n",
    "\n",
    "    This engine hooks into a target model and performs a series of analyses\n",
    "    on its activations and weights during training to identify and penalize\n",
    "    information flow bottlenecks, turbulence, and structural deficiencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, monitored_layers, homology_lambda=0.1, dmd_lambda=0.1, gft_lambda=0.1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.monitored_layers = monitored_layers\n",
    "        self.lambdas = {\n",
    "            'homology': homology_lambda,\n",
    "            'dmd': dmd_lambda,\n",
    "            'gft': gft_lambda\n",
    "        }\n",
    "\n",
    "        # 🧩 Sub-modules\n",
    "        self.recorder = ActivationRecorder(self.model, self.monitored_layers)\n",
    "        self.homology_analyzer = ChainMapHomology()\n",
    "        self.dmd_analyzer = DMDFlowAnalyzer()\n",
    "        self.graph_constructor = GraphConstructor()\n",
    "        self.spectral_analyzer = SpectralAnalyzer()\n",
    "        self.lnn_evaluator = LNNResidualEvaluator()\n",
    "        self.catastrophe_detector = CatastropheDetector()\n",
    "\n",
    "        # Data stores\n",
    "        self.activations_history = {name: [] for name in self.monitored_layers}\n",
    "\n",
    "    def _clear_history(self):\n",
    "        \"\"\"Clears the activation history, typically done each epoch.\"\"\"\n",
    "        for name in self.monitored_layers:\n",
    "            self.activations_history[name] = []\n",
    "\n",
    "    def record(self, x):\n",
    "        \"\"\"Records activations for a given input batch `x`.\"\"\"\n",
    "        activations = self.recorder(x)\n",
    "        for name, acts in activations.items():\n",
    "            # Detach from graph to avoid storing history for backprop\n",
    "            self.activations_history[name].append(acts.detach().cpu())\n",
    "        return activations\n",
    "\n",
    "    def compute_neuroflow_loss(self, current_activations):\n",
    "        \"\"\"\n",
    "        Computes the combined NeuroFlow loss after a forward pass.\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        analysis_outputs = {}\n",
    "\n",
    "        # 1. Chain Map Homology Loss (Static Structure)\n",
    "        with record_function(\"NeuroFlow: ChainMapHomology\"):\n",
    "            homology_loss, dead_zones = self.homology_analyzer(self.model, self.monitored_layers)\n",
    "            losses['homology'] = homology_loss\n",
    "            analysis_outputs['dead_zones'] = dead_zones\n",
    "\n",
    "        # For time-dependent analyses, we need sufficient history\n",
    "        if len(self.activations_history[self.monitored_layers[0]]) < 2:\n",
    "            losses['dmd'] = torch.tensor(0.0)\n",
    "            losses['gft'] = torch.tensor(0.0)\n",
    "            losses['lnn_residual'] = torch.tensor(0.0)\n",
    "            analysis_outputs['catastrophe_map'] = \"Not enough data\"\n",
    "            return losses, analysis_outputs\n",
    "\n",
    "        # Convert history to a tensor: (time, batch, features)\n",
    "        # Handle variable batch sizes by only using batches of the same size\n",
    "        history = self.activations_history[self.monitored_layers[-1]]\n",
    "        \n",
    "        # Find the minimum batch size\n",
    "        min_batch_size = min(act.shape[0] for act in history)\n",
    "        \n",
    "        # Truncate all activations to the minimum batch size\n",
    "        truncated_history = [act[:min_batch_size] for act in history]\n",
    "        \n",
    "        # Now we can safely stack\n",
    "        act_sequence = torch.stack(truncated_history)\n",
    "\n",
    "        # 2. DMD Flow Analysis (Temporal Dynamics)\n",
    "        with record_function(\"NeuroFlow: DMDFlowAnalyzer\"):\n",
    "            v, dv_dt, dmd_loss = self.dmd_analyzer(act_sequence)\n",
    "            losses['dmd'] = dmd_loss\n",
    "            analysis_outputs['velocity_field'] = v\n",
    "            analysis_outputs['acceleration_field'] = dv_dt\n",
    "\n",
    "        # 3. Graph Construction & Spectral Analysis (GFT)\n",
    "        with record_function(\"NeuroFlow: Graph & GFT\"):\n",
    "            # Use the most recent activation batch to build the graph\n",
    "            latest_activations = current_activations[self.monitored_layers[-1]]\n",
    "            adj_matrix = self.graph_constructor(latest_activations)\n",
    "            gft_loss, spectral_energy = self.spectral_analyzer(adj_matrix, latest_activations)\n",
    "            losses['gft'] = gft_loss\n",
    "            analysis_outputs['spectral_energy'] = spectral_energy\n",
    "\n",
    "        # 6. & 7. LNN Residual and Catastrophe Detection\n",
    "        with record_function(\"NeuroFlow: LNN & Catastrophe\"):\n",
    "            # Note: A full LNN implementation is complex. This is a conceptual placeholder.\n",
    "            # It would require gradients w.r.t model parameters, etc.\n",
    "            # Here we use a simplified proxy.\n",
    "            lnn_residual = self.lnn_evaluator(v, dv_dt, spectral_energy)\n",
    "            losses['lnn_residual'] = lnn_residual.mean()\n",
    "            catastrophe_map = self.catastrophe_detector(lnn_residual)\n",
    "            analysis_outputs['catastrophe_map'] = catastrophe_map\n",
    "\n",
    "        return losses, analysis_outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Main forward pass to be integrated into a training loop.\n",
    "\n",
    "        1. Records activations from the model's forward pass.\n",
    "        2. Computes the NeuroFlow diagnostic losses.\n",
    "        3. Returns a weighted total of these losses.\n",
    "        \"\"\"\n",
    "        # Note: We don't run model(x) here; we assume it's done in the\n",
    "        # main training loop. We just need the activations.\n",
    "        current_activations = self.record(x)\n",
    "\n",
    "        losses, _ = self.compute_neuroflow_loss(current_activations)\n",
    "\n",
    "        total_loss = (self.lambdas['homology'] * losses.get('homology', 0.0) +\n",
    "                      self.lambdas['dmd'] * losses.get('dmd', 0.0) +\n",
    "                      self.lambdas['gft'] * losses.get('gft', 0.0))\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# ===================================================================\n",
    "# 🧩 Module-by-Module Implementation\n",
    "# ===================================================================\n",
    "\n",
    "class ActivationRecorder(nn.Module):\n",
    "    \"\"\"\n",
    "    Hooks into a model to capture and store activations from specified layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, layer_names):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer_names = layer_names\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _hook_fn(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.activations[name] = output\n",
    "        return hook\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name in self.layer_names:\n",
    "                self.hooks.append(module.register_forward_hook(self._hook_fn(name)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.activations.clear()\n",
    "        _ = self.model(x)\n",
    "        return self.activations\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "class ChainMapHomology(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a loss penalty based on the singular value spectrum of weight matrices.\n",
    "    Aims to penalize \"thin\" or \"dead\" channels in information flow (large null space).\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, model, layer_names):\n",
    "        total_penalty = 0.0\n",
    "        dead_zones = {}\n",
    "        for name, module in model.named_modules():\n",
    "            if name in layer_names and hasattr(module, 'weight'):\n",
    "                W = module.weight\n",
    "                # Handle different weight shapes\n",
    "                if W.dim() == 4:  # Conv2d weights\n",
    "                    W = W.view(W.shape[0], -1)\n",
    "                s = torch.linalg.svdvals(W)\n",
    "                # Penalize small singular values (normalized by the largest)\n",
    "                # This encourages the matrix to be well-conditioned.\n",
    "                norm_s = s / (s.max() + self.epsilon)\n",
    "                penalty = torch.exp(-norm_s).mean()\n",
    "                total_penalty += penalty\n",
    "                dead_zones[name] = (norm_s < self.epsilon).sum().item() / len(norm_s)\n",
    "        return total_penalty, dead_zones\n",
    "\n",
    "class DMDFlowAnalyzer(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies Dynamic Mode Decomposition (DMD) to a time series of activations.\n",
    "    Extracts velocity, acceleration, and a loss based on unstable modes.\n",
    "\n",
    "    NOTE: This is a simplified DMD implementation for demonstration.\n",
    "    A production-grade version might use more robust numerical methods.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, act_sequence):\n",
    "        # act_sequence shape: [time, batch, ...] where ... could be [features] or [channels, height, width]\n",
    "        \n",
    "        # Flatten spatial dimensions if present (for conv layers)\n",
    "        if act_sequence.dim() > 3:\n",
    "            # Reshape from [time, batch, channels, height, width] to [time, batch, features]\n",
    "            time, batch = act_sequence.shape[:2]\n",
    "            act_sequence = act_sequence.view(time, batch, -1)\n",
    "        \n",
    "        # We average over the batch dimension for a global flow analysis\n",
    "        X = act_sequence.mean(dim=1).detach().cpu().numpy().T # Shape: [features, time]\n",
    "\n",
    "        # Simplified DMD\n",
    "        X1 = X[:, :-1]\n",
    "        X2 = X[:, 1:]\n",
    "\n",
    "        # Handle edge cases\n",
    "        if X1.shape[1] == 0:\n",
    "            return torch.tensor(0.0), torch.tensor(0.0), torch.tensor(0.0)\n",
    "\n",
    "        # Estimate the Koopman operator A\n",
    "        try:\n",
    "            U, S, Vt = np.linalg.svd(X1, full_matrices=False)\n",
    "            # Handle case where S might be empty or 0-dimensional\n",
    "            if S.size == 0:\n",
    "                return torch.tensor(0.0), torch.tensor(0.0), torch.tensor(0.0)\n",
    "            \n",
    "            # Add small regularization to avoid division by zero\n",
    "            S_inv = np.diag(1./(S + 1e-10))\n",
    "            A_tilde = U.T @ X2 @ Vt.T @ S_inv\n",
    "        except (np.linalg.LinAlgError, ValueError):\n",
    "            # If singular or other error, return zero loss and fields\n",
    "            return torch.tensor(0.0), torch.tensor(0.0), torch.tensor(0.0)\n",
    "\n",
    "        # Velocity is the difference between consecutive snapshots\n",
    "        v = torch.tensor(X2 - X1, dtype=torch.float32)\n",
    "        \n",
    "        # Acceleration (dv/dt) - proper calculation\n",
    "        if X1.shape[1] > 1:\n",
    "            # For acceleration, we need at least 3 time points\n",
    "            # dv/dt ≈ (v(t+1) - v(t)) / dt\n",
    "            # Since we already have v = X2 - X1, we need v_next\n",
    "            if act_sequence.shape[0] >= 3:\n",
    "                X3 = X[:, 2:]\n",
    "                v_current = X2[:, :-1] - X1[:, :-1]\n",
    "                v_next = X3 - X2[:, :-1]\n",
    "                dv_dt = torch.tensor(v_next - v_current, dtype=torch.float32)\n",
    "            else:\n",
    "                # Not enough time points for acceleration\n",
    "                dv_dt = torch.zeros_like(v)\n",
    "        else:\n",
    "            dv_dt = torch.tensor(0.)\n",
    "\n",
    "        # Eigen-decomposition of A_tilde to find dynamic modes\n",
    "        try:\n",
    "            eigvals, eigvecs = np.linalg.eig(A_tilde)\n",
    "            \n",
    "            # Loss: Penalize modes with magnitude > 1 (instability)\n",
    "            unstable_modes = np.abs(eigvals)[np.abs(eigvals) > 1.0]\n",
    "            dmd_loss = torch.tensor(np.sum(unstable_modes - 1.0), dtype=torch.float32) if len(unstable_modes) > 0 else torch.tensor(0.0)\n",
    "        except:\n",
    "            dmd_loss = torch.tensor(0.0)\n",
    "\n",
    "        return v.mean(), dv_dt.mean(), dmd_loss\n",
    "\n",
    "class GraphConstructor(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds a k-Nearest Neighbors (kNN) graph from activation data.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, features] or [batch_size, channels, height, width]\n",
    "        # Flatten if necessary\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Ensure k doesn't exceed batch size\n",
    "        k = min(self.k, x.size(0) - 1)\n",
    "        \n",
    "        if k <= 0:\n",
    "            # If batch size is 1, return a trivial adjacency matrix\n",
    "            return torch.ones(x.size(0), x.size(0), device=x.device)\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        dist_matrix = torch.cdist(x, x)\n",
    "\n",
    "        # Find k-nearest neighbors for each node\n",
    "        _, indices = torch.topk(dist_matrix, k + 1, largest=False, sorted=True)\n",
    "        \n",
    "        # Create adjacency matrix\n",
    "        adj = torch.zeros_like(dist_matrix)\n",
    "        rows = torch.arange(x.size(0)).view(-1, 1).expand(-1, k + 1)\n",
    "        adj[rows, indices] = 1\n",
    "        adj = (adj + adj.T) / 2 # Symmetrize\n",
    "        return adj\n",
    "\n",
    "\n",
    "class SpectralAnalyzer(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes Graph Fourier Transform (GFT) related metrics.\n",
    "    Estimates spectral energy and turbulence.\n",
    "    \n",
    "    NOTE: Uses full eigendecomposition for clarity. Chebyshev approximation\n",
    "    would be used for performance in a large-scale setting.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, adj_matrix, node_features):\n",
    "        # Build Graph Laplacian\n",
    "        D = torch.diag(adj_matrix.sum(dim=1))\n",
    "        L = D - adj_matrix\n",
    "\n",
    "        # Normalize Laplacian\n",
    "        D_diag = torch.diag(D)\n",
    "        D_inv_sqrt = torch.diag(1.0 / (D_diag + 1e-6).sqrt())\n",
    "        L_sym = torch.eye(L.size(0), device=L.device) - D_inv_sqrt @ L @ D_inv_sqrt\n",
    "\n",
    "        # Flatten node features\n",
    "        if node_features.dim() > 2:\n",
    "            node_features = node_features.view(node_features.size(0), -1)\n",
    "\n",
    "        # GFT: Project features onto Laplacian eigenvectors\n",
    "        # For performance, we skip the actual GFT and compute a proxy for energy.\n",
    "        # High-frequency energy is related to how \"smooth\" the signal is on the graph.\n",
    "        # This can be measured by the Rayleigh quotient: f^T L f\n",
    "        # Ensure L_sym is on the same device as node_features\n",
    "        L_sym = L_sym.to(node_features.device)\n",
    "        rayleigh_quotient = torch.diag(node_features.T @ L_sym @ node_features)\n",
    "        \n",
    "        # Loss: Penalize high spectral energy (turbulence)\n",
    "        spectral_energy = rayleigh_quotient.mean()\n",
    "        gft_loss = spectral_energy\n",
    "\n",
    "        return gft_loss, spectral_energy\n",
    "\n",
    "class LNNResidualEvaluator(nn.Module):\n",
    "    \"\"\"\n",
    "    Estimates the residual of the Neural Navier-Stokes (LNN) equation.\n",
    "    This module combines outputs from other analyzers.\n",
    "    \n",
    "    L_NN(v) = ∂_t v + (v ⋅ ∇)v - ν ∇² v - ∇L(θ) - η(t)\n",
    "    \n",
    "    NOTE: This is a conceptual stub. A full implementation is highly complex.\n",
    "    We compute a simplified residual norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, nu=0.1):\n",
    "        super().__init__()\n",
    "        self.nu = nu # Viscosity coefficient\n",
    "\n",
    "    def forward(self, v, dv_dt, spectral_energy):\n",
    "        # ∂_t v term from DMD\n",
    "        temporal_term = dv_dt\n",
    "        \n",
    "        # v ⋅ ∇v (advection) - simplified\n",
    "        advection_term = v * v # A rough proxy\n",
    "        \n",
    "        # ν ∇² v (diffusion/viscosity) term proxied by spectral energy from GFT\n",
    "        # High spectral energy corresponds to a large Laplacian quadratic form.\n",
    "        diffusion_term = self.nu * spectral_energy\n",
    "        \n",
    "        # In a full version, ∇L(θ) and η(t) would be computed via autograd and statistics\n",
    "        \n",
    "        residual_norm = torch.abs(temporal_term + advection_term - diffusion_term)\n",
    "        return residual_norm\n",
    "\n",
    "class CatastropheDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Identifies catastrophe points where the LNN residual is high.\n",
    "    These are points of high turbulence, stagnation, or singularity.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold=1.0):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, lnn_residual):\n",
    "        # lnn_residual is a tensor of residuals per point/node\n",
    "        catastrophe_map = lnn_residual > self.threshold\n",
    "        # Return the indices of the catastrophe points\n",
    "        return torch.where(catastrophe_map)[0]\n",
    "\n",
    "# ===================================================================\n",
    "# 🧪 How It All Works in Practice: A Demo on MNIST\n",
    "# ===================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Define a CNN model for MNIST\n",
    "    class MNISTModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "            self.fc1 = nn.Linear(9216, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    # --- Hyperparameters ---\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 1e-3\n",
    "    NEUROFLOW_LAMBDA_HOMOLOGY = 0.01\n",
    "    NEUROFLOW_LAMBDA_DMD = 0.001\n",
    "    NEUROFLOW_LAMBDA_GFT = 0.001\n",
    "\n",
    "    # --- Setup ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MNISTModel().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # Print layer names to verify\n",
    "    print(\"Model layers:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight'):\n",
    "            print(f\"  {name}: {module}\")\n",
    "\n",
    "    # Define the layers you want to monitor\n",
    "    monitored_layers_list = ['conv1', 'conv2']  # Monitor convolutional layers\n",
    "\n",
    "    # 2. Instantiate the NeuroFlowEngine\n",
    "    neuroflow_engine = NeuroFlowEngine(\n",
    "        model,\n",
    "        monitored_layers=monitored_layers_list,\n",
    "        homology_lambda=NEUROFLOW_LAMBDA_HOMOLOGY,\n",
    "        dmd_lambda=NEUROFLOW_LAMBDA_DMD,\n",
    "        gft_lambda=NEUROFLOW_LAMBDA_GFT\n",
    "    )\n",
    "\n",
    "    # --- Load MNIST data ---\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    print(\"🚀 Starting training with NeuroFlowEngine on MNIST...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        neuroflow_engine._clear_history() # Clear history at the start of each epoch\n",
    "        total_task_loss = 0\n",
    "        total_neuroflow_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1. Standard Forward Pass for Task Loss\n",
    "            output = model(data)\n",
    "            task_loss = criterion(output, target)\n",
    "            \n",
    "            # Track accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            # 2. NeuroFlow Analysis and Loss Calculation\n",
    "            # This runs a second forward pass internally via the recorder\n",
    "            # and then computes all diagnostic losses.\n",
    "            neuroflow_loss = neuroflow_engine(data)\n",
    "\n",
    "            # 3. Combine losses and backpropagate\n",
    "            total_loss = task_loss + neuroflow_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_task_loss += task_loss.item()\n",
    "            total_neuroflow_loss += neuroflow_loss.item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{EPOCHS} | Batch {batch_idx}/{len(train_loader)} | \"\n",
    "                      f\"Task Loss: {task_loss.item():.4f} | \"\n",
    "                      f\"NeuroFlow Loss: {neuroflow_loss.item():.4f}\")\n",
    "\n",
    "        avg_task_loss = total_task_loss / len(train_loader)\n",
    "        avg_neuroflow_loss = total_neuroflow_loss / len(train_loader)\n",
    "        accuracy = 100. * correct / len(train_loader.dataset)\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"✅ Epoch {epoch+1} Complete\")\n",
    "        print(f\"   Avg Task Loss: {avg_task_loss:.4f}\")\n",
    "        print(f\"   Avg NeuroFlow Loss: {avg_neuroflow_loss:.4f}\")\n",
    "        print(f\"   Accuracy: {accuracy:.2f}%\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # --- Post-Training Analysis ---\n",
    "    print(\"\\n🔬 Performing final analysis on the trained model...\")\n",
    "    # Get a final snapshot of diagnostics on a sample batch\n",
    "    sample_data = next(iter(train_loader))[0].to(device)\n",
    "    current_activations = neuroflow_engine.recorder(sample_data)\n",
    "    final_losses, final_analysis = neuroflow_engine.compute_neuroflow_loss(current_activations)\n",
    "\n",
    "    print(\"\\n--- Final NeuroFlow Diagnostics ---\")\n",
    "    print(f\"Homology Loss: {final_losses['homology'].item():.4f}\")\n",
    "    print(f\"  - Dead Zones per Layer: {final_analysis['dead_zones']}\")\n",
    "    print(f\"DMD (Vorticity) Loss: {final_losses['dmd'].item():.4f}\")\n",
    "    print(f\"GFT (Spectral) Loss: {final_losses['gft'].item():.4f}\")\n",
    "    print(f\"LNN Residual Norm (proxy): {final_losses['lnn_residual'].item():.4f}\")\n",
    "    print(f\"Catastrophe Map (High-Residual Nodes): {final_analysis['catastrophe_map']}\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    # Clean up hooks\n",
    "    neuroflow_engine.recorder.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "it8j1y3jets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights and NeuroFlow analysis data\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Create directory for saving\n",
    "save_dir = \"neuroflow_checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for unique filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 1. Save the trained model weights\n",
    "model_path = os.path.join(save_dir, f\"mnist_model_{timestamp}.pt\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epochs_trained': EPOCHS,\n",
    "    'final_accuracy': accuracy,\n",
    "    'architecture': {\n",
    "        'conv1': {'in_channels': 1, 'out_channels': 32, 'kernel_size': 3},\n",
    "        'conv2': {'in_channels': 32, 'out_channels': 64, 'kernel_size': 3},\n",
    "        'fc1': {'in_features': 9216, 'out_features': 128},\n",
    "        'fc2': {'in_features': 128, 'out_features': 10}\n",
    "    }\n",
    "}, model_path)\n",
    "print(f\"✅ Model weights saved to: {model_path}\")\n",
    "\n",
    "# 2. Save NeuroFlow analysis results\n",
    "analysis_path = os.path.join(save_dir, f\"neuroflow_analysis_{timestamp}.json\")\n",
    "analysis_data = {\n",
    "    'timestamp': timestamp,\n",
    "    'dataset': 'MNIST',\n",
    "    'epochs': EPOCHS,\n",
    "    'monitored_layers': monitored_layers_list,\n",
    "    'final_diagnostics': {\n",
    "        'homology_loss': float(final_losses['homology'].item()),\n",
    "        'dead_zones': final_analysis['dead_zones'],\n",
    "        'dmd_loss': float(final_losses['dmd'].item()),\n",
    "        'gft_loss': float(final_losses['gft'].item()),\n",
    "        'lnn_residual': float(final_losses['lnn_residual'].item()),\n",
    "        'catastrophe_points': len(final_analysis.get('catastrophe_map', [])) if not isinstance(final_analysis.get('catastrophe_map'), str) else 0\n",
    "    },\n",
    "    'training_config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'homology_lambda': NEUROFLOW_LAMBDA_HOMOLOGY,\n",
    "        'dmd_lambda': NEUROFLOW_LAMBDA_DMD,\n",
    "        'gft_lambda': NEUROFLOW_LAMBDA_GFT\n",
    "    },\n",
    "    'final_performance': {\n",
    "        'accuracy': accuracy,\n",
    "        'avg_task_loss': avg_task_loss,\n",
    "        'avg_neuroflow_loss': avg_neuroflow_loss\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(analysis_path, 'w') as f:\n",
    "    json.dump(analysis_data, f, indent=2)\n",
    "print(f\"✅ NeuroFlow analysis saved to: {analysis_path}\")\n",
    "\n",
    "# 3. Save activation snapshots (last batch only for memory efficiency)\n",
    "if len(neuroflow_engine.activations_history[monitored_layers_list[-1]]) > 0:\n",
    "    activation_path = os.path.join(save_dir, f\"activations_{timestamp}.npz\")\n",
    "    activation_data = {}\n",
    "    \n",
    "    for layer_name in monitored_layers_list:\n",
    "        if layer_name in neuroflow_engine.activations_history:\n",
    "            # Save only the last 5 snapshots to keep file size manageable\n",
    "            history = neuroflow_engine.activations_history[layer_name][-5:]\n",
    "            activation_data[layer_name] = [act.cpu().numpy() for act in history]\n",
    "    \n",
    "    np.savez_compressed(activation_path, **activation_data)\n",
    "    print(f\"✅ Activation snapshots saved to: {activation_path}\")\n",
    "\n",
    "# 4. Create a simple function to load the model later\n",
    "def load_neuroflow_model(checkpoint_path, device='cpu'):\n",
    "    \"\"\"Load a saved NeuroFlow model.\"\"\"\n",
    "    # Recreate model architecture\n",
    "    loaded_model = MNISTModel().to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"Model loaded from {checkpoint_path}\")\n",
    "    print(f\"Trained for {checkpoint['epochs_trained']} epochs\")\n",
    "    print(f\"Final accuracy: {checkpoint['final_accuracy']:.2%}\")\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "print(f\"\\n📁 All data saved to: {save_dir}/\")\n",
    "print(f\"📊 To load the model later, use: load_neuroflow_model('{model_path}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a49c4b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model weights saved to: neuroflow_checkpoints/mnist_model_20250712_173800.pt\n",
      "✅ NeuroFlow analysis saved to: neuroflow_checkpoints/neuroflow_analysis_20250712_173800.json\n",
      "✅ Activation snapshots saved to: neuroflow_checkpoints/activations_20250712_173800.npz\n",
      "\n",
      "📁 All data saved to: neuroflow_checkpoints/\n",
      "📊 To load the model later, use: load_neuroflow_model('neuroflow_checkpoints/mnist_model_20250712_173800.pt')\n"
     ]
    }
   ],
   "source": [
    " # Save model weights and NeuroFlow analysis data\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Create directory for saving\n",
    "save_dir = \"neuroflow_checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for unique filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 1. Save the trained model weights\n",
    "model_path = os.path.join(save_dir, f\"mnist_model_{timestamp}.pt\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epochs_trained': EPOCHS,\n",
    "    'final_accuracy': accuracy,\n",
    "    'architecture': {\n",
    "        'conv1': {'in_channels': 1, 'out_channels': 32, 'kernel_size': 3},\n",
    "        'conv2': {'in_channels': 32, 'out_channels': 64, 'kernel_size': 3},\n",
    "        'fc1': {'in_features': 9216, 'out_features': 128},\n",
    "        'fc2': {'in_features': 128, 'out_features': 10}\n",
    "    }\n",
    "}, model_path)\n",
    "print(f\"✅ Model weights saved to: {model_path}\")\n",
    "\n",
    "# 2. Save NeuroFlow analysis results\n",
    "analysis_path = os.path.join(save_dir, f\"neuroflow_analysis_{timestamp}.json\")\n",
    "analysis_data = {\n",
    "    'timestamp': timestamp,\n",
    "    'dataset': 'MNIST',\n",
    "    'epochs': EPOCHS,\n",
    "    'monitored_layers': monitored_layers_list,\n",
    "    'final_diagnostics': {\n",
    "        'homology_loss': float(final_losses['homology'].item()),\n",
    "        'dead_zones': final_analysis['dead_zones'],\n",
    "        'dmd_loss': float(final_losses['dmd'].item()),\n",
    "        'gft_loss': float(final_losses['gft'].item()),\n",
    "        'lnn_residual': float(final_losses['lnn_residual'].item()),\n",
    "        'catastrophe_points': len(final_analysis.get('catastrophe_map', [])) if not isinstance(final_analysis.get('catastrophe_map'), str) else 0\n",
    "    },\n",
    "    'training_config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'homology_lambda': NEUROFLOW_LAMBDA_HOMOLOGY,\n",
    "        'dmd_lambda': NEUROFLOW_LAMBDA_DMD,\n",
    "        'gft_lambda': NEUROFLOW_LAMBDA_GFT\n",
    "    },\n",
    "    'final_performance': {\n",
    "        'accuracy': accuracy,\n",
    "        'avg_task_loss': avg_task_loss,\n",
    "        'avg_neuroflow_loss': avg_neuroflow_loss\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(analysis_path, 'w') as f:\n",
    "    json.dump(analysis_data, f, indent=2)\n",
    "print(f\"✅ NeuroFlow analysis saved to: {analysis_path}\")\n",
    "\n",
    "# 3. Save activation snapshots (last batch only for memory efficiency)\n",
    "if len(neuroflow_engine.activations_history[monitored_layers_list[-1]]) > 0:\n",
    "    activation_path = os.path.join(save_dir, f\"activations_{timestamp}.npz\")\n",
    "    activation_data = {}\n",
    "\n",
    "    for layer_name in monitored_layers_list:\n",
    "        if layer_name in neuroflow_engine.activations_history:\n",
    "            # Save only the last 5 snapshots to keep file size manageable\n",
    "            history = neuroflow_engine.activations_history[layer_name][-5:]\n",
    "            activation_data[layer_name] = [act.cpu().numpy() for act in history]\n",
    "\n",
    "    np.savez_compressed(activation_path, **activation_data)\n",
    "    print(f\"✅ Activation snapshots saved to: {activation_path}\")\n",
    "\n",
    "# 4. Create a simple function to load the model later\n",
    "def load_neuroflow_model(checkpoint_path, device='cpu'):\n",
    "    \"\"\"Load a saved NeuroFlow model.\"\"\"\n",
    "    # Recreate model architecture\n",
    "    loaded_model = MNISTModel().to(device)\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    print(f\"Model loaded from {checkpoint_path}\")\n",
    "    print(f\"Trained for {checkpoint['epochs_trained']} epochs\")\n",
    "    print(f\"Final accuracy: {checkpoint['final_accuracy']:.2%}\")\n",
    "\n",
    "    return loaded_model\n",
    "\n",
    "print(f\"\\n📁 All data saved to: {save_dir}/\")\n",
    "print(f\"📊 To load the model later, use: load_neuroflow_model('{model_path}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0wu0d5u7x6w",
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced Visualization with Full Boundary Layer Maps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn.functional as F\nfrom matplotlib.gridspec import GridSpec\n\ndef visualize_neuroflow_analysis(model, neuroflow_engine, data_loader, device='cpu'):\n    \"\"\"\n    Enhanced visualization including full boundary layer maps for each layer.\n    \"\"\"\n    model.eval()\n    \n    # Get a sample batch\n    sample_data, sample_target = next(iter(data_loader))\n    sample_data = sample_data.to(device)\n    \n    # Run the model and collect activations\n    with torch.no_grad():\n        # First, ensure we have fresh activations\n        neuroflow_engine._clear_history()\n        \n        # Run multiple batches to build history for temporal analysis\n        for i, (data, target) in enumerate(data_loader):\n            if i >= 5:  # Collect 5 batches for temporal analysis\n                break\n            data = data.to(device)\n            _ = model(data)\n            activations = neuroflow_engine.record(data)\n        \n        # Get final batch activations for analysis\n        output = model(sample_data)\n        final_activations = neuroflow_engine.recorder(sample_data)\n    \n    # Debug: Print what we actually have\n    print(\"Available activations:\", list(final_activations.keys()))\n    print(\"Monitored layers:\", neuroflow_engine.monitored_layers)\n    \n    # Create comprehensive figure\n    fig = plt.figure(figsize=(20, 16))\n    gs = GridSpec(4, 3, figure=fig, hspace=0.3, wspace=0.3)\n    \n    # 1. Boundary Layer Maps for Each Layer (Top Row)\n    boundary_axes = []\n    for i, layer_name in enumerate(neuroflow_engine.monitored_layers[:3]):\n        ax = fig.add_subplot(gs[0, i])\n        boundary_axes.append(ax)\n        \n        if layer_name in final_activations:\n            acts = final_activations[layer_name]\n            \n            # Compute boundary maps (gradient magnitude)\n            if acts.dim() == 4:  # Conv layer\n                # Take first sample and first few channels\n                act_sample = acts[0, :8].detach().cpu()  # First 8 channels\n                \n                # Compute gradients to show boundaries\n                boundary_map = torch.zeros_like(act_sample)\n                for c in range(act_sample.shape[0]):\n                    dx = torch.abs(act_sample[c, 1:, :] - act_sample[c, :-1, :])\n                    dy = torch.abs(act_sample[c, :, 1:] - act_sample[c, :, :-1])\n                    # Pad to original size\n                    dx = F.pad(dx, (0, 0, 0, 1))\n                    dy = F.pad(dy, (0, 1, 0, 0))\n                    boundary_map[c] = dx + dy\n                \n                # Show as grid\n                n_channels = min(8, boundary_map.shape[0])\n                grid_size = int(np.ceil(np.sqrt(n_channels)))\n                for idx in range(n_channels):\n                    plt.subplot(gs[0, i])\n                    if idx == 0:\n                        im = ax.imshow(boundary_map[:n_channels].mean(0), cmap='hot')\n                        ax.set_title(f'{layer_name} Boundary Map\\n(Avg of {n_channels} channels)')\n                        plt.colorbar(im, ax=ax, fraction=0.046)\n            else:\n                # Fully connected layer - show weight magnitudes\n                ax.matshow(acts[:20, :20].abs().detach().cpu(), cmap='hot')\n                ax.set_title(f'{layer_name} Activation Magnitude')\n        else:\n            ax.text(0.5, 0.5, f'{layer_name}\\nNot Found', ha='center', va='center', transform=ax.transAxes)\n        \n        ax.set_xlabel('Spatial Dimension')\n        ax.set_ylabel('Spatial Dimension')\n    \n    # 2. Weight Matrix Analysis with SVD (Second Row)\n    ax = fig.add_subplot(gs[1, 0])\n    for name, module in model.named_modules():\n        if hasattr(module, 'weight') and name in neuroflow_engine.monitored_layers:\n            W = module.weight.detach().cpu()\n            if W.dim() == 4:  # Conv2d weights\n                W = W.view(W.shape[0], -1)\n            s = torch.linalg.svdvals(W)\n            ax.plot(s.numpy(), label=f'{name} (shape: {list(W.shape)})', marker='o', markersize=3)\n    ax.set_xlabel('Singular Value Index')\n    ax.set_ylabel('Singular Value')\n    ax.set_title('Weight Matrix Singular Values\\n(Low values indicate dead zones)')\n    ax.set_yscale('log')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # 3. Activation Flow Over Time\n    ax = fig.add_subplot(gs[1, 1])\n    if len(neuroflow_engine.activations_history[neuroflow_engine.monitored_layers[0]]) > 1:\n        for layer_name in neuroflow_engine.monitored_layers:\n            act_norms = []\n            for acts in neuroflow_engine.activations_history[layer_name]:\n                norm = acts.norm(dim=-1).mean().item()\n                act_norms.append(norm)\n            ax.plot(act_norms, label=layer_name, linewidth=2, marker='o')\n        ax.set_xlabel('Time Step')\n        ax.set_ylabel('Activation Norm')\n        ax.set_title('Information Flow Over Time\\n(Temporal Dynamics)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    \n    # 4. Layer-wise Activation Distribution\n    ax = fig.add_subplot(gs[1, 2])\n    for i, layer_name in enumerate(neuroflow_engine.monitored_layers):\n        if layer_name in final_activations:\n            acts = final_activations[layer_name].detach().cpu().flatten()\n            ax.hist(acts.numpy(), bins=50, alpha=0.6, label=layer_name, density=True)\n    ax.set_xlabel('Activation Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Activation Value Distributions')\n    ax.legend()\n    ax.set_yscale('log')\n    \n    # 5. Dead Zone Heatmap\n    ax = fig.add_subplot(gs[2, 0])\n    final_losses, final_analysis = neuroflow_engine.compute_neuroflow_loss(final_activations)\n    \n    dead_zones = final_analysis.get('dead_zones', {})\n    if dead_zones:\n        layers = list(dead_zones.keys())\n        values = list(dead_zones.values())\n        colors = ['green' if v < 0.05 else 'orange' if v < 0.1 else 'red' for v in values]\n        bars = ax.bar(layers, values, color=colors)\n        ax.set_ylabel('Dead Zone Ratio')\n        ax.set_title('Dead Zones by Layer')\n        ax.set_ylim(0, max(values) * 1.2 if values else 1)\n        \n        # Add value labels\n        for bar, val in zip(bars, values):\n            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n                   f'{val:.2%}', ha='center', va='bottom')\n    \n    # 6. Spectral Analysis\n    ax = fig.add_subplot(gs[2, 1])\n    # Build adjacency matrix for the last monitored layer\n    last_layer = neuroflow_engine.monitored_layers[-1]\n    if last_layer in final_activations:\n        latest_acts = final_activations[last_layer]\n        if latest_acts.dim() == 4:\n            latest_acts = latest_acts.mean(dim=(2, 3))\n        \n        adj_matrix = neuroflow_engine.graph_constructor(latest_acts).cpu().numpy()\n        D = np.diag(adj_matrix.sum(axis=1))\n        L = D - adj_matrix\n        \n        try:\n            eigvals = np.linalg.eigvalsh(L)[:50]\n            ax.plot(eigvals, 'g-', linewidth=2, marker='s', markersize=4)\n            ax.set_xlabel('Eigenvalue Index')\n            ax.set_ylabel('Eigenvalue')\n            ax.set_title('Graph Laplacian Spectrum\\n(Spectral Energy Distribution)')\n            ax.grid(True, alpha=0.3)\n        except:\n            ax.text(0.5, 0.5, 'Unable to compute spectrum', ha='center', va='center', transform=ax.transAxes)\n    \n    # 7. Information Flow Velocity Field\n    ax = fig.add_subplot(gs[2, 2])\n    if 'velocity_field' in final_analysis:\n        v_field = final_analysis['velocity_field']\n        if isinstance(v_field, torch.Tensor):\n            v_data = v_field.detach().cpu().numpy()\n            if v_data.ndim > 1:\n                ax.imshow(v_data[:20, :20], cmap='RdBu', aspect='auto')\n                ax.set_title('Velocity Field (DMD)')\n                ax.set_xlabel('Feature Dimension')\n                ax.set_ylabel('Feature Dimension')\n            else:\n                ax.plot(v_data)\n                ax.set_title('Velocity Field (1D)')\n    else:\n        ax.text(0.5, 0.5, 'No velocity field data', ha='center', va='center', transform=ax.transAxes)\n    \n    # 8. Full Network Activation Map\n    ax = fig.add_subplot(gs[3, :])\n    \n    # Create a combined visualization of all layers\n    all_acts = []\n    layer_labels = []\n    layer_positions = []\n    current_pos = 0\n    \n    for layer_name in neuroflow_engine.monitored_layers:\n        if layer_name in final_activations:\n            acts = final_activations[layer_name]\n            if acts.dim() == 4:\n                # For conv layers, take spatial average\n                acts = acts.mean(dim=(2, 3))\n            \n            # Take first few samples and features\n            act_slice = acts[:10, :50].abs().detach().cpu()\n            all_acts.append(act_slice)\n            layer_labels.append(layer_name)\n            layer_positions.append(current_pos + act_slice.shape[1]/2)\n            current_pos += act_slice.shape[1] + 2\n    \n    if all_acts:\n        combined_acts = torch.cat(all_acts, dim=1)\n        im = ax.imshow(combined_acts, cmap='hot', aspect='auto')\n        ax.set_title('Full Network Activation Map (First 10 Samples)')\n        ax.set_xlabel('Features (Grouped by Layer)')\n        ax.set_ylabel('Sample Index')\n        \n        # Add layer boundaries\n        current_pos = 0\n        for i, acts in enumerate(all_acts):\n            current_pos += acts.shape[1]\n            if i < len(all_acts) - 1:\n                ax.axvline(x=current_pos, color='blue', linestyle='--', alpha=0.5)\n            ax.text(layer_positions[i], -1, layer_labels[i], ha='center', va='top', rotation=45)\n        \n        plt.colorbar(im, ax=ax, fraction=0.02)\n    \n    plt.suptitle('NeuroFlow Comprehensive Analysis', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n    \n    # Print summary statistics\n    print(\"\\n📊 NeuroFlow Analysis Summary:\")\n    print(\"=\" * 50)\n    print(f\"✓ Homology Loss: {final_losses['homology'].item():.4f}\")\n    print(f\"✓ DMD Loss (Flow Instability): {final_losses['dmd'].item():.4f}\")\n    print(f\"✓ GFT Loss (Spectral Turbulence): {final_losses['gft'].item():.4f}\")\n    print(f\"✓ LNN Residual: {final_losses['lnn_residual'].item():.4f}\")\n    print(f\"✓ Dead Zones: {final_analysis['dead_zones']}\")\n    catastrophe_info = final_analysis.get('catastrophe_map', 'N/A')\n    if isinstance(catastrophe_info, str):\n        catastrophe_count = 0\n    else:\n        catastrophe_count = len(catastrophe_info) if hasattr(catastrophe_info, '__len__') else 0\n    print(f\"✓ Catastrophe Points Detected: {catastrophe_count}\")\n    print(\"=\" * 50)\n    \n    return final_losses, final_analysis\n\n# Function to load weights and visualize\ndef load_and_visualize_neuroflow(checkpoint_path, data_loader, device='cpu'):\n    \"\"\"Load saved weights and run full visualization.\"\"\"\n    # Recreate model and engine\n    model = MNISTModel().to(device)\n    monitored_layers = ['conv1', 'conv2']  # Adjust based on your model\n    neuroflow_engine = NeuroFlowEngine(\n        model,\n        monitored_layers=monitored_layers,\n        homology_lambda=0.01,\n        dmd_lambda=0.001,\n        gft_lambda=0.001\n    )\n    \n    # Load checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    \n    print(f\"✅ Model loaded from {checkpoint_path}\")\n    print(f\"   Trained for {checkpoint['epochs_trained']} epochs\")\n    print(f\"   Final accuracy: {checkpoint['final_accuracy']:.2%}\")\n    \n    # Run visualization\n    losses, analysis = visualize_neuroflow_analysis(model, neuroflow_engine, data_loader, device)\n    \n    return model, neuroflow_engine, losses, analysis\n\n# Run the enhanced visualization\nprint(\"📊 Generating enhanced NeuroFlow visualizations with boundary maps...\")\nlosses, analysis = visualize_neuroflow_analysis(model, neuroflow_engine, train_loader, device)"
  },
  {
   "cell_type": "code",
   "id": "tk1ladpobk",
   "source": "# Example: Load previously saved weights and visualize\n# Uncomment and update the path to use your saved checkpoint\n\n# checkpoint_path = \"neuroflow_checkpoints/mnist_model_20241112_143022.pt\"  # Update with your actual filename\n# loaded_model, loaded_engine, loaded_losses, loaded_analysis = load_and_visualize_neuroflow(\n#     checkpoint_path, \n#     train_loader, \n#     device\n# )\n\n# You can also create individual boundary layer visualizations\ndef visualize_layer_boundaries(model, layer_name, data_loader, device='cpu', num_samples=4):\n    \"\"\"Visualize boundary maps for a specific layer with multiple samples.\"\"\"\n    model.eval()\n    \n    # Get activation from specific layer\n    activations = {}\n    def hook_fn(module, input, output):\n        activations['output'] = output\n    \n    # Register hook\n    target_module = None\n    for name, module in model.named_modules():\n        if name == layer_name:\n            target_module = module\n            hook = module.register_forward_hook(hook_fn)\n            break\n    \n    if target_module is None:\n        print(f\"Layer {layer_name} not found!\")\n        return\n    \n    # Get data\n    data, _ = next(iter(data_loader))\n    data = data[:num_samples].to(device)\n    \n    with torch.no_grad():\n        _ = model(data)\n    \n    # Remove hook\n    hook.remove()\n    \n    # Visualize boundaries\n    if 'output' in activations:\n        acts = activations['output'].detach().cpu()\n        \n        if acts.dim() == 4:  # Conv layer\n            fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n            if num_samples == 1:\n                axes = axes.reshape(1, -1)\n            \n            for sample_idx in range(num_samples):\n                # Original input\n                ax = axes[sample_idx, 0]\n                ax.imshow(data[sample_idx, 0].cpu(), cmap='gray')\n                ax.set_title(f'Input Sample {sample_idx}')\n                ax.axis('off')\n                \n                # Mean activation\n                ax = axes[sample_idx, 1]\n                mean_act = acts[sample_idx].mean(0)\n                im = ax.imshow(mean_act, cmap='hot')\n                ax.set_title(f'Mean Activation')\n                ax.axis('off')\n                plt.colorbar(im, ax=ax, fraction=0.046)\n                \n                # Boundary map\n                ax = axes[sample_idx, 2]\n                boundary = torch.zeros_like(mean_act)\n                dx = torch.abs(mean_act[1:, :] - mean_act[:-1, :])\n                dy = torch.abs(mean_act[:, 1:] - mean_act[:, :-1])\n                boundary[:-1, :] += dx\n                boundary[:, :-1] += dy\n                im = ax.imshow(boundary, cmap='hot')\n                ax.set_title('Boundary Map')\n                ax.axis('off')\n                plt.colorbar(im, ax=ax, fraction=0.046)\n                \n                # Max activation across channels\n                ax = axes[sample_idx, 3]\n                max_act, _ = acts[sample_idx].max(0)\n                im = ax.imshow(max_act, cmap='hot')\n                ax.set_title('Max Channel Response')\n                ax.axis('off')\n                plt.colorbar(im, ax=ax, fraction=0.046)\n            \n            plt.suptitle(f'Boundary Analysis for {layer_name}', fontsize=16)\n            plt.tight_layout()\n            plt.show()\n        else:\n            print(f\"Layer {layer_name} is not a convolutional layer\")\n\n# Visualize boundaries for each monitored layer\nfor layer in ['conv1', 'conv2']:\n    print(f\"\\n🔍 Analyzing boundaries for {layer}...\")\n    visualize_layer_boundaries(model, layer, train_loader, device, num_samples=3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6kq3g1q2ur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional MNIST-specific visualizations\n",
    "def visualize_mnist_predictions(model, data_loader, device='cpu', num_samples=10):\n",
    "    \"\"\"\n",
    "    Visualize MNIST predictions with NeuroFlow analysis overlays\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of data\n",
    "    data, target = next(iter(data_loader))\n",
    "    data, target = data[:num_samples].to(device), target[:num_samples].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "    fig.suptitle('MNIST Predictions with NeuroFlow Analysis', fontsize=14)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_samples:\n",
    "            # Show the image\n",
    "            img = data[i].cpu().squeeze()\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            \n",
    "            # Add prediction info\n",
    "            true_label = target[i].item()\n",
    "            pred_label = pred[i].item()\n",
    "            confidence = torch.exp(output[i, pred_label]).item()\n",
    "            \n",
    "            color = 'green' if pred_label == true_label else 'red'\n",
    "            ax.set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.2f}', \n",
    "                        color=color, fontsize=10)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample predictions\n",
    "print(\"🔍 Visualizing MNIST predictions...\")\n",
    "visualize_mnist_predictions(model, train_loader, device)\n",
    "\n",
    "# Show training curves if we collected them\n",
    "def plot_training_history():\n",
    "    \"\"\"\n",
    "    Plot training curves from the training loop\n",
    "    \"\"\"\n",
    "    # This would normally use data collected during training\n",
    "    # For demo purposes, we'll create synthetic curves\n",
    "    epochs = np.arange(1, 6)\n",
    "    task_loss = np.array([2.3, 1.8, 1.4, 1.1, 0.9])\n",
    "    neuroflow_loss = np.array([0.05, 0.04, 0.03, 0.025, 0.02])\n",
    "    accuracy = np.array([85, 90, 92, 94, 95])\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Task loss\n",
    "    ax1.plot(epochs, task_loss, 'b-', marker='o', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Task Loss')\n",
    "    ax1.set_title('Classification Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # NeuroFlow loss\n",
    "    ax2.plot(epochs, neuroflow_loss, 'r-', marker='s', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('NeuroFlow Loss')\n",
    "    ax2.set_title('Information Flow Diagnostics')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    ax3.plot(epochs, accuracy, 'g-', marker='^', linewidth=2)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.set_title('MNIST Accuracy')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Training Progress with NeuroFlow Monitoring', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n📈 Training progress visualization...\")\n",
    "plot_training_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}