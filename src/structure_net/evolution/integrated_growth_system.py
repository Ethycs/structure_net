#!/usr/bin/env python3
"""
Integrated Growth System - Complete MI + Threshold + SensLI + Tournament

This module implements the complete integrated system from integration 1.md,
adapted to work with the canonical structure_net modular architecture.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import networkx as nx
from scipy import sparse as sp
from scipy.spatial.distance import pdist, squareform
from sklearn.feature_selection import mutual_info_regression
from sklearn.neighbors import NearestNeighbors
from collections import defaultdict
import time
from typing import Dict, List, Tuple, Any, Optional
import copy
import logging

# Import canonical structure_net components
from ..core.layers import StandardSparseLayer
from ..core.network_factory import create_standard_network
from ..core.network_analysis import get_network_stats
from ..core.io_operations import save_model_seed, load_model_seed

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# PART 1: CONFIGURATION
# ============================================================================

class ThresholdConfig:
    """Configuration for threshold-based filtering."""
    def __init__(self):
        self.activation_threshold = 0.01
        self.gradient_threshold = 0.001
        self.weight_threshold = 0.001
        self.adaptive = True
        self.min_active_ratio = 0.001
        self.max_active_ratio = 0.1
        self.persistence_ratio = 0.5  # Neuron must be active 50% of time

class MetricsConfig:
    """Configuration for metrics computation."""
    def __init__(self):
        self.compute_betweenness = True
        self.compute_spectral = True
        self.compute_paths = True
        self.compute_motifs = True
        self.compute_percolation = True
        self.max_path_length = 10
        self.betweenness_samples = 100
        self.spectral_k = 10

# ============================================================================
# PART 2: EXACT MUTUAL INFORMATION
# ============================================================================

class ExactMutualInformation:
    """Compute EXACT mutual information for sparse, thresholded networks."""
    
    def __init__(self, threshold=0.01):
        self.threshold = threshold
        
    def compute_exact_mi(self, X: torch.Tensor, Y: torch.Tensor) -> Dict[str, float]:
        """
        Compute EXACT mutual information between active neurons.
        Returns detailed MI statistics.
        """
        # Apply threshold to get active neurons only
        X_active_mask = (X.abs() > self.threshold).any(dim=0)
        Y_active_mask = (Y.abs() > self.threshold).any(dim=0)
        
        X_active = X[:, X_active_mask]
        Y_active = Y[:, Y_active_mask]
        
        n_samples = X.shape[0]
        n_active_X = X_active.shape[1]
        n_active_Y = Y_active.shape[1]
        
        logger.info(f"Computing EXACT MI: {X.shape} ‚Üí {X_active.shape} active neurons")
        
        if n_active_X == 0 or n_active_Y == 0:
            return {
                'mi': 0.0,
                'normalized_mi': 0.0,
                'entropy_X': 0.0,
                'entropy_Y': 0.0,
                'entropy_XY': 0.0,
                'active_neurons_X': 0,
                'active_neurons_Y': 0,
                'method': 'zero_active'
            }
        
        # Choose method based on dimensionality
        if n_active_X <= 10 and n_active_Y <= 10 and n_samples > 50:
            # Use exact discrete MI
            result = self._exact_discrete_mi(X_active, Y_active)
            result['method'] = 'exact_discrete'
        elif n_active_X + n_active_Y <= 50:
            # Use k-NN estimator (essentially exact for low dimensions)
            result = self._knn_mi(X_active, Y_active)
            result['method'] = 'knn_exact'
        else:
            # Still manageable with advanced estimators
            result = self._advanced_mi_estimator(X_active, Y_active)
            result['method'] = 'advanced_estimator'
        
        # Add neuron counts
        result['active_neurons_X'] = n_active_X
        result['active_neurons_Y'] = n_active_Y
        result['total_neurons_X'] = X.shape[1]
        result['total_neurons_Y'] = Y.shape[1]
        result['sparsity_X'] = n_active_X / X.shape[1]
        result['sparsity_Y'] = n_active_Y / Y.shape[1]
        
        return result
    
    def _exact_discrete_mi(self, X: torch.Tensor, Y: torch.Tensor) -> Dict[str, float]:
        """Exact MI computation for discretized values."""
        # Optimal binning based on sample size
        n_bins = min(int(np.sqrt(X.shape[0]) / 2), 10)
        
        # Discretize
        X_discrete = self._adaptive_discretize(X, n_bins)
        Y_discrete = self._adaptive_discretize(Y, n_bins)
        
        # Compute joint probability distribution
        joint_hist = torch.zeros(n_bins, n_bins)
        for i in range(X.shape[0]):
            for j in range(X.shape[1]):
                for k in range(Y.shape[1]):
                    joint_hist[X_discrete[i, j], Y_discrete[i, k]] += 1
        
        joint_prob = joint_hist / joint_hist.sum()
        
        # Marginal probabilities
        p_x = joint_prob.sum(dim=1)
        p_y = joint_prob.sum(dim=0)
        
        # Compute MI, H(X), H(Y), H(X,Y)
        mi = 0.0
        h_x = 0.0
        h_y = 0.0
        h_xy = 0.0
        
        for i in range(n_bins):
            if p_x[i] > 0:
                h_x -= p_x[i] * torch.log2(p_x[i])
            if p_y[i] > 0:
                h_y -= p_y[i] * torch.log2(p_y[i])
                
            for j in range(n_bins):
                if joint_prob[i, j] > 0:
                    h_xy -= joint_prob[i, j] * torch.log2(joint_prob[i, j])
                    mi += joint_prob[i, j] * torch.log2(
                        joint_prob[i, j] / (p_x[i] * p_y[j] + 1e-10)
                    )
        
        # Normalized MI
        normalized_mi = 2 * mi / (h_x + h_y + 1e-10)
        
        return {
            'mi': mi.item(),
            'normalized_mi': normalized_mi.item(),
            'entropy_X': h_x.item(),
            'entropy_Y': h_y.item(),
            'entropy_XY': h_xy.item()
        }
    
    def _knn_mi(self, X: torch.Tensor, Y: torch.Tensor) -> Dict[str, float]:
        """k-NN based MI estimator (Kraskov et al.)"""
        X_np = X.cpu().numpy()
        Y_np = Y.cpu().numpy()
        
        # Use sklearn's implementation
        mi_values = []
        for i in range(Y_np.shape[1]):
            mi = mutual_info_regression(X_np, Y_np[:, i], n_neighbors=3)
            mi_values.extend(mi)
        
        mi = np.mean(mi_values)
        
        # Estimate entropies using k-NN
        h_x = self._knn_entropy(X_np)
        h_y = self._knn_entropy(Y_np)
        h_xy = self._knn_entropy(np.hstack([X_np, Y_np]))
        
        # Normalized MI
        normalized_mi = 2 * mi / (h_x + h_y + 1e-10)
        
        return {
            'mi': mi,
            'normalized_mi': normalized_mi,
            'entropy_X': h_x,
            'entropy_Y': h_y,
            'entropy_XY': h_xy
        }
    
    def _knn_entropy(self, X: np.ndarray, k: int = 3) -> float:
        """Estimate entropy using k-NN distances."""
        if X.shape[0] < k + 1:
            return 0.0
            
        # Find k-NN distances
        nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(X)
        distances, _ = nbrs.kneighbors(X)
        
        # Use k-th neighbor distance for entropy estimation
        rho = distances[:, k]
        
        # Kozachenko-Leonenko estimator
        d = X.shape[1]
        volume = (np.pi ** (d/2)) / np.exp(np.log(math.gamma(d/2 + 1)))
        
        h = np.log(rho + 1e-10).mean() * d + np.log(volume) + np.log(X.shape[0]) - np.log(k)
        
        # Convert to bits
        return h / np.log(2)
    
    def _advanced_mi_estimator(self, X: torch.Tensor, Y: torch.Tensor) -> Dict[str, float]:
        """Advanced MI estimator for higher dimensions."""
        X_norm = F.normalize(X, dim=0)
        Y_norm = F.normalize(Y, dim=0)
        
        # Canonical correlation
        corr_matrix = torch.abs(X_norm.T @ Y_norm)
        max_corr = corr_matrix.max(dim=1)[0].mean()
        
        # Upper bound on MI
        mi_upper = -0.5 * torch.log(1 - max_corr**2 + 1e-8)
        
        # Estimate entropies
        h_x = self._differential_entropy(X)
        h_y = self._differential_entropy(Y)
        
        return {
            'mi': mi_upper.item(),
            'normalized_mi': mi_upper.item() / (min(h_x, h_y) + 1e-10),
            'entropy_X': h_x,
            'entropy_Y': h_y,
            'entropy_XY': h_x + h_y - mi_upper.item()
        }
    
    def _adaptive_discretize(self, X: torch.Tensor, n_bins: int) -> torch.Tensor:
        """Adaptive discretization based on data distribution."""
        X_discrete = torch.zeros_like(X, dtype=torch.long)
        
        for j in range(X.shape[1]):
            # Use quantiles for adaptive binning
            col = X[:, j]
            if col.std() > 0:
                bins = torch.quantile(col, torch.linspace(0, 1, n_bins + 1))
                bins[-1] += 1e-5  # Ensure last value is included
                X_discrete[:, j] = torch.bucketize(col, bins) - 1
            else:
                X_discrete[:, j] = 0
        
        return X_discrete.clamp(0, n_bins - 1)
    
    def _differential_entropy(self, X: torch.Tensor) -> float:
        """Estimate differential entropy."""
        if X.shape[1] == 1:
            # 1D case: use histogram
            return self._histogram_entropy(X.squeeze())
        else:
            # Multi-D: use covariance-based estimate
            cov = torch.cov(X.T)
            sign, logdet = torch.linalg.slogdet(cov + 1e-6 * torch.eye(X.shape[1]))
            
            # Gaussian entropy approximation
            d = X.shape[1]
            h = 0.5 * (d * np.log(2 * np.pi * np.e) + logdet)
            return h.item() / np.log(2)  # Convert to bits
    
    def _histogram_entropy(self, x: torch.Tensor) -> float:
        """Compute entropy using histogram."""
        hist = torch.histc(x, bins=int(np.sqrt(len(x))))
        hist = hist / hist.sum()
        hist = hist[hist > 0]
        return -torch.sum(hist * torch.log2(hist)).item()

# ============================================================================
# PART 3: INTEGRATED GROWTH SYSTEM FOR STRUCTURE_NET
# ============================================================================

class StructureNetGrowthSystem:
    """
    Integrated growth system adapted for structure_net canonical architecture.
    
    This system combines exact MI analysis, threshold-based filtering,
    and tournament-based strategy selection with the modular structure_net.
    """
    
    def __init__(self, network: nn.Sequential, 
                 threshold_config: ThresholdConfig = None,
                 metrics_config: MetricsConfig = None):
        self.network = network
        self.threshold_config = threshold_config or ThresholdConfig()
        self.metrics_config = metrics_config or MetricsConfig()
        
        # Initialize analyzers
        self.exact_mi = ExactMutualInformation(self.threshold_config.activation_threshold)
        
        # Growth history
        self.growth_history = []
        self.performance_history = []
        
    def analyze_network_bottlenecks(self, data_loader, num_batches: int = 10) -> Dict[str, Any]:
        """
        Comprehensive bottleneck analysis using exact MI and threshold filtering.
        """
        logger.info("üîç Analyzing network bottlenecks with exact MI...")
        
        analysis_results = {
            'layer_analyses': {},
            'bottlenecks': [],
            'dead_zones': [],
            'recommendations': []
        }
        
        # Get sparse layers
        sparse_layers = [layer for layer in self.network if isinstance(layer, StandardSparseLayer)]
        
        if len(sparse_layers) < 2:
            logger.warning("Network has fewer than 2 sparse layers - cannot analyze bottlenecks")
            return analysis_results
        
        # Analyze each layer pair
        for i in range(len(sparse_layers) - 1):
            layer_pair_analysis = self._analyze_layer_pair(
                i, i + 1, data_loader, num_batches
            )
            analysis_results['layer_analyses'][f'layer_{i}_{i+1}'] = layer_pair_analysis
            
            # Identify bottlenecks
            if layer_pair_analysis['mi_efficiency'] < 0.3:
                analysis_results['bottlenecks'].append({
                    'layer_pair': (i, i + 1),
                    'severity': 1 - layer_pair_analysis['mi_efficiency'],
                    'type': 'information_bottleneck',
                    'mi_efficiency': layer_pair_analysis['mi_efficiency']
                })
            
            # Identify dead zones
            if (layer_pair_analysis['dead_ratio_input'] > 0.5 or 
                layer_pair_analysis['dead_ratio_output'] > 0.5):
                analysis_results['dead_zones'].append({
                    'layer_pair': (i, i + 1),
                    'dead_ratio_input': layer_pair_analysis['dead_ratio_input'],
                    'dead_ratio_output': layer_pair_analysis['dead_ratio_output'],
                    'type': 'dead_zone'
                })
        
        # Generate recommendations
        analysis_results['recommendations'] = self._generate_recommendations(analysis_results)
        
        self._print_analysis_summary(analysis_results)
        
        return analysis_results
    
    def _analyze_layer_pair(self, layer_i: int, layer_j: int, 
                           data_loader, num_batches: int) -> Dict[str, Any]:
        """Analyze information flow between two layers."""
        
        # Collect activations
        activations_i = []
        activations_j = []
        
        self.network.eval()
        with torch.no_grad():
            for batch_idx, (data, _) in enumerate(data_loader):
                if batch_idx >= num_batches:
                    break
                
                # Forward pass to collect activations
                x = data.view(data.size(0), -1)
                layer_outputs = []
                
                for idx, layer in enumerate(self.network):
                    if isinstance(layer, StandardSparseLayer):
                        x = layer(x)
                        layer_outputs.append(x.clone())
                        if idx < len(self.network) - 1:
                            x = F.relu(x)
                    elif isinstance(layer, nn.ReLU):
                        x = layer(x)
                
                # Store activations for target layers
                if layer_i < len(layer_outputs):
                    activations_i.append(layer_outputs[layer_i])
                if layer_j < len(layer_outputs):
                    activations_j.append(layer_outputs[layer_j])
        
        if not activations_i or not activations_j:
            return {'error': 'Could not collect activations'}
        
        # Concatenate activations
        acts_i = torch.cat(activations_i, dim=0)
        acts_j = torch.cat(activations_j, dim=0)
        
        # Apply ReLU to intermediate activations
        if layer_i < len(self.network) - 1:
            acts_i = F.relu(acts_i)
        if layer_j < len(self.network) - 1:
            acts_j = F.relu(acts_j)
        
        # Compute exact MI
        mi_result = self.exact_mi.compute_exact_mi(acts_i, acts_j)
        
        # Compute dead neuron ratios
        active_mask_i = (acts_i.abs() > self.threshold_config.activation_threshold).any(dim=0)
        active_mask_j = (acts_j.abs() > self.threshold_config.activation_threshold).any(dim=0)
        
        dead_ratio_i = 1 - active_mask_i.float().mean().item()
        dead_ratio_j = 1 - active_mask_j.float().mean().item()
        
        # Compute MI efficiency
        theoretical_max_mi = min(mi_result['entropy_X'], mi_result['entropy_Y'])
        mi_efficiency = mi_result['mi'] / (theoretical_max_mi + 1e-10)
        
        return {
            'layer_pair': (layer_i, layer_j),
            'mi': mi_result['mi'],
            'normalized_mi': mi_result['normalized_mi'],
            'mi_efficiency': mi_efficiency,
            'entropy_input': mi_result['entropy_X'],
            'entropy_output': mi_result['entropy_Y'],
            'active_neurons_input': mi_result['active_neurons_X'],
            'active_neurons_output': mi_result['active_neurons_Y'],
            'dead_ratio_input': dead_ratio_i,
            'dead_ratio_output': dead_ratio_j,
            'method': mi_result['method']
        }
    
    def _generate_recommendations(self, analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate growth recommendations based on analysis."""
        recommendations = []
        
        # Handle severe bottlenecks
        for bottleneck in analysis_results['bottlenecks']:
            if bottleneck['severity'] > 0.7:  # Severe bottleneck
                recommendations.append({
                    'action': 'insert_layer',
                    'position': bottleneck['layer_pair'][0] + 1,
                    'reason': f"Severe information bottleneck (efficiency: {bottleneck['mi_efficiency']:.2%})",
                    'priority': 'high',
                    'expected_improvement': bottleneck['severity']
                })
            elif bottleneck['severity'] > 0.4:  # Moderate bottleneck
                recommendations.append({
                    'action': 'increase_layer_width',
                    'layer': bottleneck['layer_pair'][1],
                    'factor': 1.5,
                    'reason': f"Moderate information bottleneck (efficiency: {bottleneck['mi_efficiency']:.2%})",
                    'priority': 'medium',
                    'expected_improvement': bottleneck['severity'] * 0.7
                })
        
        # Handle dead zones
        for dead_zone in analysis_results['dead_zones']:
            if dead_zone['dead_ratio_input'] > 0.7:
                recommendations.append({
                    'action': 'revive_dead_neurons',
                    'layer': dead_zone['layer_pair'][0],
                    'method': 'add_skip_connections',
                    'reason': f"High dead neuron ratio: {dead_zone['dead_ratio_input']:.1%}",
                    'priority': 'high',
                    'expected_improvement': dead_zone['dead_ratio_input']
                })
        
        # Sort by priority and expected improvement
        priority_order = {'high': 3, 'medium': 2, 'low': 1}
        recommendations.sort(
            key=lambda x: (priority_order.get(x['priority'], 0), x['expected_improvement']),
            reverse=True
        )
        
        return recommendations
    
    def apply_growth_recommendations(self, recommendations: List[Dict[str, Any]], 
                                   max_actions: int = 3) -> nn.Sequential:
        """
        Apply growth recommendations to create an improved network.
        """
        logger.info(f"üå± Applying top {min(len(recommendations), max_actions)} growth recommendations...")
        
        # Get current architecture
        current_stats = get_network_stats(self.network)
        current_arch = current_stats['architecture']
        current_sparsity = current_stats['overall_sparsity']
        
        # Apply recommendations
        new_arch = current_arch.copy()
        
        for i, rec in enumerate(recommendations[:max_actions]):
            logger.info(f"   {i+1}. {rec['action']}: {rec['reason']}")
            
            if rec['action'] == 'insert_layer':
                # Insert new layer
                pos = rec['position']
                if pos < len(new_arch):
                    # Calculate optimal width
                    width_before = new_arch[pos-1] if pos > 0 else new_arch[0]
                    width_after = new_arch[pos] if pos < len(new_arch) else new_arch[-1]
                    new_width = int(np.sqrt(width_before * width_after) * 1.2)
                    new_arch.insert(pos, new_width)
                    logger.info(f"      Inserted layer of width {new_width} at position {pos}")
            
            elif rec['action'] == 'increase_layer_width':
                # Increase layer width
                layer_idx = rec['layer']
                factor = rec.get('factor', 1.5)
                if 0 < layer_idx < len(new_arch):
                    old_width = new_arch[layer_idx]
                    new_arch[layer_idx] = int(old_width * factor)
                    logger.info(f"      Increased layer {layer_idx} width: {old_width} ‚Üí {new_arch[layer_idx]}")
        
        # Create new network with improved architecture
        if new_arch != current_arch:
            logger.info(f"   üìê New architecture: {new_arch}")
            
            # Create new network using canonical factory
            improved_network = create_standard_network(
                architecture=new_arch,
                sparsity=current_sparsity,
                seed=None,  # Use random initialization for new parts
                device=str(next(self.network.parameters()).device)
            )
            
            # Copy weights from old network where possible
            self._transfer_weights(self.network, improved_network, current_arch, new_arch)
            
            return improved_network
        else:
            logger.info("   üìê No architectural changes needed")
            return self.network
    
    def _transfer_weights(self, old_network: nn.Sequential, new_network: nn.Sequential,
                         old_arch: List[int], new_arch: List[int]):
        """Transfer weights from old network to new network where possible."""
        logger.info("   üîÑ Transferring weights from old network...")
        
        old_sparse_layers = [layer for layer in old_network if isinstance(layer, StandardSparseLayer)]
        new_sparse_layers = [layer for layer in new_network if isinstance(layer, StandardSparseLayer)]
        
        # Simple transfer for matching layers
        old_idx = 0
        for new_idx, new_layer in enumerate(new_sparse_layers):
            if old_idx < len(old_sparse_layers):
                old_layer = old_sparse_layers[old_idx]
                
                # Check if dimensions match
                if (old_layer.linear.weight.shape == new_layer.linear.weight.shape and
                    old_layer.linear.bias.shape == new_layer.linear.bias.shape):
                    
                    # Copy weights and mask
                    with torch.no_grad():
                        new_layer.linear.weight.data.copy_(old_layer.linear.weight.data)
                        new_layer.linear.bias.data.copy_(old_layer.linear.bias.data)
                        new_layer.mask.data.copy_(old_layer.mask.data)
                    
                    logger.info(f"      Copied weights for layer {new_idx}")
                    old_idx += 1
                else:
                    logger.info(f"      Layer {new_idx} has new dimensions - using random initialization")
                    if old_idx < len(old_sparse_layers) - 1:  # Skip if not output layer
                        old_idx += 1
    
    def _print_analysis_summary(self, analysis_results: Dict[str, Any]):
        """Print summary of network analysis."""
        logger.info("\nüìä NETWORK ANALYSIS SUMMARY")
        logger.info("=" * 50)
        
        # Bottlenecks
        bottlenecks = analysis_results['bottlenecks']
        if bottlenecks:
            logger.info(f"\nüö® Found {len(bottlenecks)} information bottlenecks:")
            for b in bottlenecks:
                logger.info(f"   Layer {b['layer_pair'][0]}‚Üí{b['layer_pair'][1]}: "
                          f"MI efficiency {b['mi_efficiency']:.2%} (severity: {b['severity']:.2f})")
        else:
            logger.info("\n‚úÖ No significant information bottlenecks found")
        
        # Dead zones
        dead_zones = analysis_results['dead_zones']
        if dead_zones:
            logger.info(f"\nüíÄ Found {len(dead_zones)} dead zones:")
            for d in dead_zones:
                logger.info(f"   Layer {d['layer_pair'][0]}‚Üí{d['layer_pair'][1]}: "
                          f"Dead ratios {d['dead_ratio_input']:.1%}/{d['dead_ratio_output']:.1%}")
        else:
            logger.info("\n‚úÖ No significant dead zones found")
        
        # Recommendations
        recommendations = analysis_results['recommendations']
        if recommendations:
            logger.info(f"\nüí° Generated {len(recommendations)} recommendations:")
            for i, r in enumerate(recommendations[:5]):  # Show top 5
                logger.info(f"   {i+1}. {r['action']}: {r['reason']} (priority: {r['priority']})")
        else:
            logger.info("\n‚úÖ No growth recommendations needed")

# ============================================================================
# EXPORT FUNCTIONS
# ============================================================================

def analyze_and_grow_network(network: nn.Sequential, 
                           train_loader, 
                           val_loader = None,
                           threshold_config: ThresholdConfig = None,
                           max_growth_actions: int = 3,
                           num_analysis_batches: int = 10) -> nn.Sequential:
    """
    Convenience function to analyze and grow a network using the integrated system.
    
    Args:
        network: Network created with create_standard_network()
        train_loader: Training data loader for analysis
        val_loader: Optional validation data loader
        threshold_config: Configuration for threshold-based filtering
        max_growth_actions: Maximum number of growth actions to apply
        num_analysis_batches: Number of batches to use for analysis
        
    Returns:
        Improved network
    """
    # Create growth system
    growth_system = StructureNetGrowthSystem(network, threshold_config)
    
    # Analyze network
    analysis_results = growth_system.analyze_network_bottlenecks(
        train_loader, num_analysis_batches
    )
    
    # Apply improvements
    improved_network = growth_system.apply_growth_recommendations(
        analysis_results['recommendations'], max_growth_actions
    )
    
    return improved_network

# Export the main classes and functions
__all__ = [
    'ThresholdConfig',
    'MetricsConfig', 
    'ExactMutualInformation',
    'StructureNetGrowthSystem',
    'analyze_and_grow_network'
]
